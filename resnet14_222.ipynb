{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nM4iedsjAt6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
        "from torch.utils import model_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI7FfpQVw53P"
      },
      "outputs": [],
      "source": [
        "train_policy = transforms.Compose([\n",
        "                    transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], # specific to CIFAR-10, from https://towardsdatascience.com/data-augmentations-in-torchvision-5d56d70c372e\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "\n",
        "test_policy = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "                ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "6a7c69a34ba94d419000c814ca20be6d",
            "6909966f928040fa979d882985a777b3",
            "c6b4ae84acc845fd94f92476e79dbabc",
            "78d3b5af789e4096859d7f4c307cbc69",
            "4adfa694ca1040899aed6a26be808540",
            "d0616d758ae84331af5376c7007e455a",
            "5a0cdd5f0a4e44a6b73b3e578522317a",
            "337ba3f2b64b483eb15a7c2769b27e17",
            "5354924b45f5478d9e49d04fdc6d7a92",
            "c6c40a11513947ebbcbfe3f3e6cb82d0",
            "f13b6d03be264bdfbd13b4013d64514d"
          ]
        },
        "id": "TRtP734ljNJh",
        "outputId": "0eaf0ea7-6393-4d8f-cbf1-e89dcd3f87b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a7c69a34ba94d419000c814ca20be6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_policy)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=40, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_policy)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=40, shuffle=False, num_workers=2)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWdTEY4D3k92"
      },
      "source": [
        "# Defining the Model\n",
        "## Credit: Took lots of code from https://github.com/pytorch/vision/blob/a9a8220e0bcb4ce66a733f8c03a1c2f6c68d22cb/torchvision/models/resnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijLWPa9e3jTx"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "        #                                dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5IX4Yz-jOnm"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# model = ResNet(BasicBlock, [5, 3, 3])\n",
        "model = ResNet(BasicBlock, [2, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcahYVJR1oI8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB01cvSG2nnW",
        "outputId": "32a84a2c-f6e5-4731-cbe8-b4f16011a773"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3039784"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDYs76at2xCY"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, iteration_count,lr=5e-4,lr_decay=5e-6):\n",
        "    lr = lr / (1.0 + lr_decay * iteration_count)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b__ZT7kX_Se"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self):\n",
        "    self.loss_t = []\n",
        "    self.steps_t = []\n",
        "    self.acc_t = []\n",
        "    self.global_step = 0\n",
        "    self.epoch = 0\n",
        "\n",
        "  def train(self, max_epoch):\n",
        "    for self.epoch in range(max_epoch):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        self.global_step += 1\n",
        "        adjust_learning_rate(optimizer, self.global_step)\n",
        "        model.cuda()\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i%400 == 399:\n",
        "          self.loss_t.append(running_loss)\n",
        "          self.steps_t.append(self.global_step)\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "            for data in testloader:\n",
        "              images, labels = data\n",
        "              images, labels = images.to(device), labels.to(device)\n",
        "              outputs = model(images)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "              accu = 100 * correct / total\n",
        "          print('[%d, %5d] loss: %.3f accuracy: %d %%' % (self.epoch + 1, self.global_step, running_loss / 400, (accu)))\n",
        "          print('lr',optimizer.param_groups[0]['lr'])\n",
        "          self.acc_t.append(accu)\n",
        "          running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh_Q71XkYAYx"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgocteh-YI3L",
        "outputId": "72f7923b-b80b-4050-fcf9-9d6df0e56fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   400] loss: 2.651 accuracy: 36 %\n",
            "lr 0.000499001996007984\n",
            "[1,   800] loss: 1.907 accuracy: 41 %\n",
            "lr 0.00049800796812749\n",
            "[1,  1200] loss: 1.787 accuracy: 45 %\n",
            "lr 0.0004970178926441352\n",
            "[2,  1650] loss: 1.697 accuracy: 47 %\n",
            "lr 0.0004959087527894867\n",
            "[2,  2050] loss: 1.651 accuracy: 49 %\n",
            "lr 0.0004949269982677555\n",
            "[2,  2450] loss: 1.585 accuracy: 51 %\n",
            "lr 0.0004939491232403062\n",
            "[3,  2900] loss: 1.542 accuracy: 54 %\n",
            "lr 0.0004928536224741252\n",
            "[3,  3300] loss: 1.495 accuracy: 54 %\n",
            "lr 0.0004918839153959666\n",
            "[3,  3700] loss: 1.460 accuracy: 56 %\n",
            "lr 0.0004909180166912126\n",
            "[4,  4150] loss: 1.413 accuracy: 56 %\n",
            "lr 0.0004898359049718344\n",
            "[4,  4550] loss: 1.393 accuracy: 57 %\n",
            "lr 0.0004888780249327792\n",
            "[4,  4950] loss: 1.383 accuracy: 59 %\n",
            "lr 0.00048792388387411563\n",
            "[5,  5400] loss: 1.327 accuracy: 59 %\n",
            "lr 0.00048685491723466414\n",
            "[5,  5800] loss: 1.310 accuracy: 60 %\n",
            "lr 0.0004859086491739553\n",
            "[5,  6200] loss: 1.297 accuracy: 61 %\n",
            "lr 0.0004849660523763337\n",
            "[6,  6650] loss: 1.243 accuracy: 62 %\n",
            "lr 0.0004839099927413501\n",
            "[6,  7050] loss: 1.268 accuracy: 61 %\n",
            "lr 0.0004829751267809708\n",
            "[6,  7450] loss: 1.246 accuracy: 63 %\n",
            "lr 0.00048204386599180526\n",
            "[7,  7900] loss: 1.179 accuracy: 63 %\n",
            "lr 0.00048100048100048096\n",
            "[7,  8300] loss: 1.208 accuracy: 63 %\n",
            "lr 0.00048007681228996637\n",
            "[7,  8700] loss: 1.166 accuracy: 65 %\n",
            "lr 0.00047915668423574505\n",
            "[8,  9150] loss: 1.153 accuracy: 64 %\n",
            "lr 0.00047812574707147985\n",
            "[8,  9550] loss: 1.145 accuracy: 65 %\n",
            "lr 0.00047721307563827254\n",
            "[8,  9950] loss: 1.138 accuracy: 66 %\n",
            "lr 0.00047630388187663733\n",
            "[9, 10400] loss: 1.110 accuracy: 66 %\n",
            "lr 0.0004752851711026616\n",
            "[9, 10800] loss: 1.104 accuracy: 67 %\n",
            "lr 0.0004743833017077799\n",
            "[9, 11200] loss: 1.084 accuracy: 66 %\n",
            "lr 0.0004734848484848485\n",
            "[10, 11650] loss: 1.050 accuracy: 67 %\n",
            "lr 0.00047247814788566034\n",
            "[10, 12050] loss: 1.059 accuracy: 67 %\n",
            "lr 0.0004715868898844613\n",
            "[10, 12450] loss: 1.061 accuracy: 68 %\n",
            "lr 0.00047069898799717585\n",
            "[11, 12900] loss: 1.037 accuracy: 67 %\n",
            "lr 0.0004697040864255519\n",
            "[11, 13300] loss: 1.038 accuracy: 68 %\n",
            "lr 0.00046882325363338024\n",
            "[11, 13700] loss: 1.021 accuracy: 68 %\n",
            "lr 0.0004679457182966776\n",
            "[12, 14150] loss: 0.996 accuracy: 68 %\n",
            "lr 0.0004669624095260331\n",
            "[12, 14550] loss: 0.978 accuracy: 69 %\n",
            "lr 0.0004660918200885574\n",
            "[12, 14950] loss: 0.994 accuracy: 68 %\n",
            "lr 0.00046522447080716444\n",
            "[13, 15400] loss: 0.970 accuracy: 69 %\n",
            "lr 0.00046425255338904364\n",
            "[13, 15800] loss: 0.978 accuracy: 69 %\n",
            "lr 0.0004633920296570899\n",
            "[13, 16200] loss: 0.946 accuracy: 69 %\n",
            "lr 0.00046253469010175765\n",
            "[14, 16650] loss: 0.944 accuracy: 70 %\n",
            "lr 0.0004615739672282483\n",
            "[14, 17050] loss: 0.942 accuracy: 69 %\n",
            "lr 0.00046072333563695\n",
            "[14, 17450] loss: 0.946 accuracy: 69 %\n",
            "lr 0.0004598758335249483\n",
            "[15, 17900] loss: 0.907 accuracy: 70 %\n",
            "lr 0.00045892611289582384\n",
            "[15, 18300] loss: 0.905 accuracy: 71 %\n",
            "lr 0.00045808520384791576\n",
            "[15, 18700] loss: 0.916 accuracy: 70 %\n",
            "lr 0.0004572473708276178\n",
            "[16, 19150] loss: 0.887 accuracy: 70 %\n",
            "lr 0.0004563084645220169\n",
            "[16, 19550] loss: 0.877 accuracy: 69 %\n",
            "lr 0.0004554771122751082\n",
            "[16, 19950] loss: 0.904 accuracy: 70 %\n",
            "lr 0.0004546487838145033\n",
            "[17, 20400] loss: 0.845 accuracy: 70 %\n",
            "lr 0.00045372050816696913\n",
            "[17, 20800] loss: 0.870 accuracy: 70 %\n",
            "lr 0.00045289855072463763\n",
            "[17, 21200] loss: 0.872 accuracy: 70 %\n",
            "lr 0.0004520795660036166\n",
            "[18, 21650] loss: 0.828 accuracy: 70 %\n",
            "lr 0.0004511617414843222\n",
            "[18, 22050] loss: 0.835 accuracy: 71 %\n",
            "lr 0.0004503490204908805\n",
            "[18, 22450] loss: 0.856 accuracy: 70 %\n",
            "lr 0.0004495392222971455\n",
            "[19, 22900] loss: 0.811 accuracy: 71 %\n",
            "lr 0.00044863167339614175\n",
            "[19, 23300] loss: 0.824 accuracy: 71 %\n",
            "lr 0.0004478280340349306\n",
            "[19, 23700] loss: 0.834 accuracy: 71 %\n",
            "lr 0.0004470272686633885\n",
            "[20, 24150] loss: 0.798 accuracy: 71 %\n",
            "lr 0.00044612982377871963\n",
            "[20, 24550] loss: 0.794 accuracy: 71 %\n",
            "lr 0.00044533511467379205\n",
            "[20, 24950] loss: 0.802 accuracy: 71 %\n",
            "lr 0.00044454323182929544\n",
            "[21, 25400] loss: 0.768 accuracy: 71 %\n",
            "lr 0.00044365572315882877\n",
            "[21, 25800] loss: 0.768 accuracy: 72 %\n",
            "lr 0.0004428697962798937\n",
            "[21, 26200] loss: 0.766 accuracy: 72 %\n",
            "lr 0.00044208664898320074\n",
            "[22, 26650] loss: 0.751 accuracy: 72 %\n",
            "lr 0.00044120891242003087\n",
            "[22, 27050] loss: 0.758 accuracy: 72 %\n",
            "lr 0.0004404316229905307\n",
            "[22, 27450] loss: 0.758 accuracy: 71 %\n",
            "lr 0.0004396570674873598\n",
            "[23, 27900] loss: 0.737 accuracy: 72 %\n",
            "lr 0.00043878894251864854\n",
            "[23, 28300] loss: 0.735 accuracy: 72 %\n",
            "lr 0.0004380201489268507\n",
            "[23, 28700] loss: 0.745 accuracy: 72 %\n",
            "lr 0.00043725404459991256\n",
            "[24, 29150] loss: 0.715 accuracy: 72 %\n",
            "lr 0.0004363953742090334\n",
            "[24, 29550] loss: 0.711 accuracy: 72 %\n",
            "lr 0.00043563493792202136\n",
            "[24, 29950] loss: 0.728 accuracy: 72 %\n",
            "lr 0.0004348771472059143\n",
            "[25, 30400] loss: 0.711 accuracy: 72 %\n",
            "lr 0.00043402777777777775\n",
            "[25, 30800] loss: 0.702 accuracy: 72 %\n",
            "lr 0.00043327556325823227\n",
            "[25, 31200] loss: 0.722 accuracy: 72 %\n",
            "lr 0.0004325259515570935\n",
            "[26, 31650] loss: 0.697 accuracy: 72 %\n",
            "lr 0.0004316857327865314\n",
            "[26, 32050] loss: 0.686 accuracy: 72 %\n",
            "lr 0.00043094160741219563\n",
            "[26, 32450] loss: 0.686 accuracy: 72 %\n",
            "lr 0.00043020004302000433\n",
            "[27, 32900] loss: 0.669 accuracy: 72 %\n",
            "lr 0.0004293688278231\n",
            "[27, 33300] loss: 0.667 accuracy: 72 %\n",
            "lr 0.0004286326618088298\n",
            "[27, 33700] loss: 0.683 accuracy: 72 %\n",
            "lr 0.00042789901583226354\n",
            "[28, 34150] loss: 0.663 accuracy: 72 %\n",
            "lr 0.0004270766602605168\n",
            "[28, 34550] loss: 0.662 accuracy: 72 %\n",
            "lr 0.0004263483265828182\n",
            "[28, 34950] loss: 0.671 accuracy: 72 %\n",
            "lr 0.00042562247286656737\n",
            "[29, 35400] loss: 0.656 accuracy: 72 %\n",
            "lr 0.0004248088360237893\n",
            "[29, 35800] loss: 0.642 accuracy: 72 %\n",
            "lr 0.00042408821034775233\n",
            "[29, 36200] loss: 0.658 accuracy: 72 %\n",
            "lr 0.0004233700254022015\n",
            "[30, 36650] loss: 0.632 accuracy: 73 %\n",
            "lr 0.00042256496936403967\n",
            "[30, 37050] loss: 0.637 accuracy: 73 %\n",
            "lr 0.0004218519299725797\n",
            "[30, 37450] loss: 0.634 accuracy: 73 %\n",
            "lr 0.00042114129290376915\n",
            "[31, 37900] loss: 0.618 accuracy: 72 %\n",
            "lr 0.0004203446826397646\n",
            "[31, 38300] loss: 0.629 accuracy: 72 %\n",
            "lr 0.000419639110365086\n",
            "[31, 38700] loss: 0.614 accuracy: 73 %\n",
            "lr 0.00041893590280687055\n",
            "[32, 39150] loss: 0.594 accuracy: 72 %\n",
            "lr 0.000418147606104955\n",
            "[32, 39550] loss: 0.618 accuracy: 73 %\n",
            "lr 0.0004174493842621582\n",
            "[32, 39950] loss: 0.627 accuracy: 72 %\n",
            "lr 0.0004167534903104813\n",
            "[33, 40400] loss: 0.584 accuracy: 72 %\n",
            "lr 0.00041597337770382697\n",
            "[33, 40800] loss: 0.594 accuracy: 72 %\n",
            "lr 0.0004152823920265781\n",
            "[33, 41200] loss: 0.599 accuracy: 73 %\n",
            "lr 0.00041459369817578774\n",
            "[34, 41650] loss: 0.579 accuracy: 73 %\n",
            "lr 0.0004138216428719222\n",
            "[34, 42050] loss: 0.585 accuracy: 73 %\n",
            "lr 0.0004131377814501136\n",
            "[34, 42450] loss: 0.596 accuracy: 73 %\n",
            "lr 0.00041245617653124357\n",
            "[35, 42900] loss: 0.553 accuracy: 73 %\n",
            "lr 0.0004116920543433511\n",
            "[35, 43300] loss: 0.570 accuracy: 73 %\n",
            "lr 0.00041101520756267986\n",
            "[35, 43700] loss: 0.570 accuracy: 73 %\n",
            "lr 0.00041034058268362735\n",
            "[36, 44150] loss: 0.549 accuracy: 73 %\n",
            "lr 0.0004095842719639566\n",
            "[36, 44550] loss: 0.574 accuracy: 73 %\n",
            "lr 0.0004089143324473523\n",
            "[36, 44950] loss: 0.575 accuracy: 73 %\n",
            "lr 0.0004082465809348847\n",
            "[37, 45400] loss: 0.542 accuracy: 73 %\n",
            "lr 0.00040749796251018743\n",
            "[37, 45800] loss: 0.551 accuracy: 73 %\n",
            "lr 0.0004068348250610252\n",
            "[37, 46200] loss: 0.569 accuracy: 73 %\n",
            "lr 0.00040617384240454913\n",
            "[38, 46650] loss: 0.530 accuracy: 73 %\n",
            "lr 0.00040543279951348065\n",
            "[38, 47050] loss: 0.535 accuracy: 73 %\n",
            "lr 0.0004047763610605141\n",
            "[38, 47450] loss: 0.552 accuracy: 73 %\n",
            "lr 0.000404122044857547\n",
            "[39, 47900] loss: 0.539 accuracy: 73 %\n",
            "lr 0.0004033884630899556\n",
            "[39, 48300] loss: 0.530 accuracy: 73 %\n",
            "lr 0.0004027386226339106\n",
            "[39, 48700] loss: 0.549 accuracy: 74 %\n",
            "lr 0.0004020908725371934\n",
            "[40, 49150] loss: 0.543 accuracy: 73 %\n",
            "lr 0.00040136463977523575\n",
            "[40, 49550] loss: 0.525 accuracy: 73 %\n",
            "lr 0.00040072129833700667\n",
            "[40, 49950] loss: 0.541 accuracy: 73 %\n",
            "lr 0.0004000800160032006\n",
            "[41, 50400] loss: 0.510 accuracy: 74 %\n",
            "lr 0.00039936102236421724\n",
            "[41, 50800] loss: 0.534 accuracy: 74 %\n",
            "lr 0.00039872408293460925\n",
            "[41, 51200] loss: 0.508 accuracy: 73 %\n",
            "lr 0.0003980891719745223\n",
            "[42, 51650] loss: 0.505 accuracy: 74 %\n",
            "lr 0.00039737730975561295\n",
            "[42, 52050] loss: 0.502 accuracy: 73 %\n",
            "lr 0.00039674667724657806\n",
            "[42, 52450] loss: 0.518 accuracy: 73 %\n",
            "lr 0.0003961180431768667\n",
            "[43, 52900] loss: 0.501 accuracy: 73 %\n",
            "lr 0.00039541320680110717\n",
            "[43, 53300] loss: 0.511 accuracy: 74 %\n",
            "lr 0.0003947887879984209\n",
            "[43, 53700] loss: 0.511 accuracy: 74 %\n",
            "lr 0.0003941663381947182\n",
            "[44, 54150] loss: 0.479 accuracy: 73 %\n",
            "lr 0.0003934684241589612\n",
            "[44, 54550] loss: 0.499 accuracy: 73 %\n",
            "lr 0.0003928501276762915\n",
            "[44, 54950] loss: 0.505 accuracy: 74 %\n",
            "lr 0.0003922337713277113\n",
            "[45, 55400] loss: 0.503 accuracy: 74 %\n",
            "lr 0.0003915426781519185\n",
            "[45, 55800] loss: 0.493 accuracy: 73 %\n",
            "lr 0.0003909304143862393\n",
            "[45, 56200] loss: 0.508 accuracy: 74 %\n",
            "lr 0.00039032006245120994\n",
            "[46, 56650] loss: 0.491 accuracy: 74 %\n",
            "lr 0.00038963569062926167\n",
            "[46, 57050] loss: 0.482 accuracy: 74 %\n",
            "lr 0.0003890293717175647\n",
            "[46, 57450] loss: 0.481 accuracy: 73 %\n",
            "lr 0.0003884249368809478\n",
            "[47, 57900] loss: 0.486 accuracy: 74 %\n",
            "lr 0.00038774718883288094\n",
            "[47, 58300] loss: 0.468 accuracy: 73 %\n",
            "lr 0.00038714672861014324\n",
            "[47, 58700] loss: 0.495 accuracy: 73 %\n",
            "lr 0.00038654812524159255\n",
            "[48, 59150] loss: 0.469 accuracy: 73 %\n",
            "lr 0.00038587690526721975\n",
            "[48, 59550] loss: 0.473 accuracy: 73 %\n",
            "lr 0.00038528221922558276\n",
            "[48, 59950] loss: 0.484 accuracy: 74 %\n",
            "lr 0.0003846893633391037\n",
            "[49, 60400] loss: 0.469 accuracy: 73 %\n",
            "lr 0.00038402457757296467\n",
            "[49, 60800] loss: 0.477 accuracy: 74 %\n",
            "lr 0.0003834355828220859\n",
            "[49, 61200] loss: 0.470 accuracy: 74 %\n",
            "lr 0.00038284839203675346\n",
            "[50, 61650] loss: 0.451 accuracy: 73 %\n",
            "lr 0.00038218994840435696\n",
            "[50, 62050] loss: 0.472 accuracy: 74 %\n",
            "lr 0.0003816065636328945\n",
            "[50, 62450] loss: 0.465 accuracy: 73 %\n",
            "lr 0.0003810249571346923\n",
            "[51, 62900] loss: 0.452 accuracy: 74 %\n",
            "lr 0.0003803727653100038\n",
            "[51, 63300] loss: 0.459 accuracy: 74 %\n",
            "lr 0.00037979491074819596\n",
            "[51, 63700] loss: 0.457 accuracy: 74 %\n",
            "lr 0.0003792188092529389\n",
            "[52, 64150] loss: 0.449 accuracy: 72 %\n",
            "lr 0.0003785727806170736\n",
            "[52, 64550] loss: 0.459 accuracy: 74 %\n",
            "lr 0.000378000378000378\n",
            "[52, 64950] loss: 0.449 accuracy: 73 %\n",
            "lr 0.0003774297037176826\n",
            "[53, 65400] loss: 0.439 accuracy: 74 %\n",
            "lr 0.00037678975131876413\n",
            "[53, 65800] loss: 0.438 accuracy: 74 %\n",
            "lr 0.00037622272385252073\n",
            "[53, 66200] loss: 0.451 accuracy: 74 %\n",
            "lr 0.0003756574004507889\n",
            "[54, 66650] loss: 0.444 accuracy: 74 %\n",
            "lr 0.0003750234389649353\n",
            "[54, 67050] loss: 0.437 accuracy: 74 %\n",
            "lr 0.00037446171129002057\n",
            "[54, 67450] loss: 0.449 accuracy: 74 %\n",
            "lr 0.0003739016638624042\n",
            "[55, 67900] loss: 0.439 accuracy: 74 %\n",
            "lr 0.00037327360955580435\n",
            "[55, 68300] loss: 0.448 accuracy: 74 %\n",
            "lr 0.00037271710771524417\n",
            "[55, 68700] loss: 0.439 accuracy: 74 %\n",
            "lr 0.00037216226274655747\n",
            "[56, 69150] loss: 0.431 accuracy: 74 %\n",
            "lr 0.000371540033438603\n",
            "[56, 69550] loss: 0.439 accuracy: 73 %\n",
            "lr 0.00037098868484511225\n",
            "[56, 69950] loss: 0.442 accuracy: 74 %\n",
            "lr 0.0003704389701796629\n",
            "[57, 70400] loss: 0.443 accuracy: 74 %\n",
            "lr 0.0003698224852071006\n",
            "[57, 70800] loss: 0.424 accuracy: 74 %\n",
            "lr 0.0003692762186115214\n",
            "[57, 71200] loss: 0.424 accuracy: 74 %\n",
            "lr 0.0003687315634218289\n",
            "[58, 71650] loss: 0.414 accuracy: 74 %\n",
            "lr 0.0003681207436039021\n",
            "[58, 72050] loss: 0.420 accuracy: 74 %\n",
            "lr 0.00036757948906451023\n",
            "[58, 72450] loss: 0.438 accuracy: 75 %\n",
            "lr 0.0003670398238208846\n",
            "[59, 72900] loss: 0.421 accuracy: 74 %\n",
            "lr 0.00036643459142543056\n",
            "[59, 73300] loss: 0.435 accuracy: 75 %\n",
            "lr 0.00036589828027808267\n",
            "[59, 73700] loss: 0.434 accuracy: 75 %\n",
            "lr 0.00036536353671903543\n",
            "[60, 74150] loss: 0.412 accuracy: 74 %\n",
            "lr 0.0003647638154295094\n",
            "[60, 74550] loss: 0.421 accuracy: 74 %\n",
            "lr 0.000364232380258605\n",
            "[60, 74950] loss: 0.428 accuracy: 74 %\n",
            "lr 0.0003637024913620658\n",
            "[61, 75400] loss: 0.412 accuracy: 74 %\n",
            "lr 0.00036310820624546115\n",
            "[61, 75800] loss: 0.409 accuracy: 74 %\n",
            "lr 0.00036258158085569254\n",
            "[61, 76200] loss: 0.415 accuracy: 74 %\n",
            "lr 0.00036205648081100655\n",
            "[62, 76650] loss: 0.408 accuracy: 74 %\n",
            "lr 0.0003614675582866438\n",
            "[62, 77050] loss: 0.412 accuracy: 75 %\n",
            "lr 0.00036094567767550983\n",
            "[62, 77450] loss: 0.405 accuracy: 74 %\n",
            "lr 0.00036042530185619026\n",
            "[63, 77900] loss: 0.416 accuracy: 74 %\n",
            "lr 0.0003598416696653473\n",
            "[63, 78300] loss: 0.413 accuracy: 74 %\n",
            "lr 0.00035932446999640676\n",
            "[63, 78700] loss: 0.401 accuracy: 75 %\n",
            "lr 0.0003588087549336204\n",
            "[64, 79150] loss: 0.404 accuracy: 75 %\n",
            "lr 0.0003582303421099767\n",
            "[64, 79550] loss: 0.420 accuracy: 75 %\n",
            "lr 0.0003577177606868181\n",
            "[64, 79950] loss: 0.407 accuracy: 74 %\n",
            "lr 0.0003572066440435792\n",
            "[65, 80400] loss: 0.415 accuracy: 74 %\n",
            "lr 0.00035663338088445074\n",
            "[65, 80800] loss: 0.413 accuracy: 75 %\n",
            "lr 0.00035612535612535614\n",
            "[65, 81200] loss: 0.399 accuracy: 75 %\n",
            "lr 0.0003556187766714082\n",
            "[66, 81650] loss: 0.404 accuracy: 74 %\n",
            "lr 0.00035505059470974617\n",
            "[66, 82050] loss: 0.399 accuracy: 74 %\n",
            "lr 0.0003545470661230278\n",
            "[66, 82450] loss: 0.414 accuracy: 75 %\n",
            "lr 0.00035404496371039125\n",
            "[67, 82900] loss: 0.397 accuracy: 75 %\n",
            "lr 0.00035348179568752205\n",
            "[67, 83300] loss: 0.394 accuracy: 75 %\n",
            "lr 0.00035298270384751147\n",
            "[67, 83700] loss: 0.399 accuracy: 74 %\n",
            "lr 0.00035248501938667606\n",
            "[68, 84150] loss: 0.378 accuracy: 75 %\n",
            "lr 0.00035192679922576106\n",
            "[68, 84550] loss: 0.396 accuracy: 74 %\n",
            "lr 0.0003514320857494289\n",
            "[68, 84950] loss: 0.399 accuracy: 74 %\n",
            "lr 0.00035093876118617303\n",
            "[69, 85400] loss: 0.396 accuracy: 75 %\n",
            "lr 0.000350385423966363\n",
            "[69, 85800] loss: 0.382 accuracy: 75 %\n",
            "lr 0.0003498950314905528\n",
            "[69, 86200] loss: 0.400 accuracy: 75 %\n",
            "lr 0.00034940600978336826\n",
            "[70, 86650] loss: 0.393 accuracy: 74 %\n",
            "lr 0.00034885749171463456\n",
            "[70, 87050] loss: 0.393 accuracy: 74 %\n",
            "lr 0.0003483713638738896\n",
            "[70, 87450] loss: 0.400 accuracy: 74 %\n",
            "lr 0.0003478865889719951\n",
            "[71, 87900] loss: 0.381 accuracy: 74 %\n",
            "lr 0.0003473428273706148\n",
            "[71, 88300] loss: 0.386 accuracy: 75 %\n",
            "lr 0.000346860908775581\n",
            "[71, 88700] loss: 0.401 accuracy: 75 %\n",
            "lr 0.00034638032559750607\n",
            "[72, 89150] loss: 0.378 accuracy: 75 %\n",
            "lr 0.00034584125886218223\n",
            "[72, 89550] loss: 0.371 accuracy: 75 %\n",
            "lr 0.00034536349507857016\n",
            "[72, 89950] loss: 0.389 accuracy: 75 %\n",
            "lr 0.0003448870494912916\n",
            "[73, 90400] loss: 0.387 accuracy: 75 %\n",
            "lr 0.0003443526170798898\n",
            "[73, 90800] loss: 0.391 accuracy: 75 %\n",
            "lr 0.000343878954607978\n",
            "[73, 91200] loss: 0.382 accuracy: 74 %\n",
            "lr 0.00034340659340659343\n",
            "[74, 91650] loss: 0.374 accuracy: 75 %\n",
            "lr 0.000342876735813475\n",
            "[74, 92050] loss: 0.364 accuracy: 75 %\n",
            "lr 0.000342407122068139\n",
            "[74, 92450] loss: 0.387 accuracy: 74 %\n",
            "lr 0.00034193879295606086\n",
            "[75, 92900] loss: 0.378 accuracy: 75 %\n",
            "lr 0.00034141345168999654\n",
            "[75, 93300] loss: 0.363 accuracy: 75 %\n",
            "lr 0.0003409478349812479\n",
            "[75, 93700] loss: 0.380 accuracy: 75 %\n",
            "lr 0.00034048348655090226\n",
            "[76, 94150] loss: 0.371 accuracy: 75 %\n",
            "lr 0.0003399626041135475\n",
            "[76, 94550] loss: 0.360 accuracy: 75 %\n",
            "lr 0.00033950093362756747\n",
            "[76, 94950] loss: 0.383 accuracy: 75 %\n",
            "lr 0.00033904051534158335\n",
            "[77, 95400] loss: 0.366 accuracy: 75 %\n",
            "lr 0.00033852403520649965\n",
            "[77, 95800] loss: 0.375 accuracy: 75 %\n",
            "lr 0.0003380662609871535\n",
            "[77, 96200] loss: 0.378 accuracy: 75 %\n",
            "lr 0.000337609723160027\n",
            "[78, 96650] loss: 0.377 accuracy: 75 %\n",
            "lr 0.0003370975897522333\n",
            "[78, 97050] loss: 0.368 accuracy: 75 %\n",
            "lr 0.00033664366268305\n",
            "[78, 97450] loss: 0.375 accuracy: 75 %\n",
            "lr 0.00033619095646327115\n",
            "[79, 97900] loss: 0.375 accuracy: 75 %\n",
            "lr 0.0003356831151393085\n",
            "[79, 98300] loss: 0.355 accuracy: 75 %\n",
            "lr 0.00033523298692591353\n",
            "[79, 98700] loss: 0.368 accuracy: 75 %\n",
            "lr 0.00033478406427854036\n",
            "[80, 99150] loss: 0.363 accuracy: 75 %\n",
            "lr 0.0003342804613070366\n",
            "[80, 99550] loss: 0.362 accuracy: 75 %\n",
            "lr 0.00033383408446002337\n",
            "[80, 99950] loss: 0.367 accuracy: 75 %\n",
            "lr 0.0003333888981496916\n",
            "[81, 100400] loss: 0.361 accuracy: 75 %\n",
            "lr 0.00033288948069241014\n",
            "[81, 100800] loss: 0.355 accuracy: 75 %\n",
            "lr 0.0003324468085106383\n",
            "[81, 101200] loss: 0.378 accuracy: 75 %\n",
            "lr 0.0003320053120849934\n",
            "[82, 101650] loss: 0.369 accuracy: 75 %\n",
            "lr 0.00033151002817835237\n",
            "[82, 102050] loss: 0.355 accuracy: 75 %\n",
            "lr 0.00033107101473266014\n",
            "[82, 102450] loss: 0.373 accuracy: 75 %\n",
            "lr 0.00033063316250619935\n",
            "[83, 102900] loss: 0.355 accuracy: 75 %\n",
            "lr 0.0003301419610432486\n",
            "[83, 103300] loss: 0.350 accuracy: 75 %\n",
            "lr 0.00032970656116056705\n",
            "[83, 103700] loss: 0.367 accuracy: 75 %\n",
            "lr 0.0003292723081988805\n",
            "[84, 104150] loss: 0.353 accuracy: 75 %\n",
            "lr 0.0003287851389117212\n",
            "[84, 104550] loss: 0.361 accuracy: 75 %\n",
            "lr 0.0003283533081595797\n",
            "[84, 104950] loss: 0.354 accuracy: 75 %\n",
            "lr 0.0003279226102639777\n",
            "[85, 105400] loss: 0.359 accuracy: 75 %\n",
            "lr 0.00032743942370661423\n",
            "[85, 105800] loss: 0.370 accuracy: 75 %\n",
            "lr 0.0003270111183780249\n",
            "[85, 106200] loss: 0.353 accuracy: 75 %\n",
            "lr 0.0003265839320705421\n",
            "[86, 106650] loss: 0.355 accuracy: 75 %\n",
            "lr 0.0003261046796021523\n",
            "[86, 107050] loss: 0.358 accuracy: 76 %\n",
            "lr 0.0003256798567008631\n",
            "[86, 107450] loss: 0.357 accuracy: 75 %\n",
            "lr 0.0003252561392096276\n",
            "[87, 107900] loss: 0.350 accuracy: 75 %\n",
            "lr 0.0003247807729782397\n",
            "[87, 108300] loss: 0.358 accuracy: 75 %\n",
            "lr 0.0003243593902043464\n",
            "[87, 108700] loss: 0.359 accuracy: 75 %\n",
            "lr 0.00032393909944930353\n",
            "[88, 109150] loss: 0.367 accuracy: 75 %\n",
            "lr 0.00032346757237586933\n",
            "[88, 109550] loss: 0.355 accuracy: 75 %\n",
            "lr 0.0003230495881117751\n",
            "[88, 109950] loss: 0.351 accuracy: 75 %\n",
            "lr 0.00032263268269075657\n",
            "[89, 110400] loss: 0.340 accuracy: 75 %\n",
            "lr 0.00032216494845360824\n",
            "[89, 110800] loss: 0.357 accuracy: 76 %\n",
            "lr 0.00032175032175032174\n",
            "[89, 111200] loss: 0.370 accuracy: 75 %\n",
            "lr 0.00032133676092544985\n",
            "[90, 111650] loss: 0.334 accuracy: 75 %\n",
            "lr 0.00032087277394513073\n",
            "[90, 112050] loss: 0.361 accuracy: 75 %\n",
            "lr 0.00032046146450889285\n",
            "[90, 112450] loss: 0.343 accuracy: 75 %\n",
            "lr 0.0003200512081933109\n",
            "[91, 112900] loss: 0.342 accuracy: 75 %\n",
            "lr 0.00031959092361776926\n",
            "[91, 113300] loss: 0.346 accuracy: 76 %\n",
            "lr 0.0003191828917969997\n",
            "[91, 113700] loss: 0.349 accuracy: 75 %\n",
            "lr 0.000318775900541919\n",
            "[92, 114150] loss: 0.343 accuracy: 75 %\n",
            "lr 0.0003183192742320547\n",
            "[92, 114550] loss: 0.349 accuracy: 75 %\n",
            "lr 0.00031791448100460976\n",
            "[92, 114950] loss: 0.351 accuracy: 75 %\n",
            "lr 0.00031751071598666456\n",
            "[93, 115400] loss: 0.350 accuracy: 75 %\n",
            "lr 0.0003170577045022194\n",
            "[93, 115800] loss: 0.349 accuracy: 75 %\n",
            "lr 0.0003166561114629512\n",
            "[93, 116200] loss: 0.352 accuracy: 75 %\n",
            "lr 0.00031625553447185326\n",
            "[94, 116650] loss: 0.348 accuracy: 75 %\n",
            "lr 0.00031580609505763463\n",
            "[94, 117050] loss: 0.341 accuracy: 75 %\n",
            "lr 0.00031540766440624505\n",
            "[94, 117450] loss: 0.344 accuracy: 75 %\n",
            "lr 0.00031501023783272954\n",
            "[95, 117900] loss: 0.345 accuracy: 75 %\n",
            "lr 0.00031456432840515884\n",
            "[95, 118300] loss: 0.333 accuracy: 75 %\n",
            "lr 0.0003141690229343387\n",
            "[95, 118700] loss: 0.334 accuracy: 75 %\n",
            "lr 0.00031377470975839345\n",
            "[96, 119150] loss: 0.340 accuracy: 75 %\n",
            "lr 0.00031333228889237035\n",
            "[96, 119550] loss: 0.339 accuracy: 75 %\n",
            "lr 0.00031294007197621654\n",
            "[96, 119950] loss: 0.349 accuracy: 75 %\n",
            "lr 0.0003125488357555868\n",
            "[97, 120400] loss: 0.340 accuracy: 76 %\n",
            "lr 0.00031210986267166043\n",
            "[97, 120800] loss: 0.347 accuracy: 75 %\n",
            "lr 0.0003117206982543641\n",
            "[97, 121200] loss: 0.331 accuracy: 75 %\n",
            "lr 0.00031133250311332503\n",
            "[98, 121650] loss: 0.325 accuracy: 75 %\n",
            "lr 0.000310896937665164\n",
            "[98, 122050] loss: 0.335 accuracy: 75 %\n",
            "lr 0.0003105107902499612\n",
            "[98, 122450] loss: 0.334 accuracy: 75 %\n",
            "lr 0.0003101256008683517\n",
            "[99, 122900] loss: 0.332 accuracy: 75 %\n",
            "lr 0.0003096934035305048\n",
            "[99, 123300] loss: 0.335 accuracy: 75 %\n",
            "lr 0.0003093102381688834\n",
            "[99, 123700] loss: 0.339 accuracy: 75 %\n",
            "lr 0.00030892801977139327\n",
            "[100, 124150] loss: 0.316 accuracy: 75 %\n",
            "lr 0.000308499151627333\n",
            "[100, 124550] loss: 0.322 accuracy: 75 %\n",
            "lr 0.0003081189339084887\n",
            "[100, 124950] loss: 0.321 accuracy: 75 %\n",
            "lr 0.00030773965225419295\n",
            "[101, 125400] loss: 0.332 accuracy: 75 %\n",
            "lr 0.0003073140749846343\n",
            "[101, 125800] loss: 0.336 accuracy: 75 %\n",
            "lr 0.00030693677102516884\n",
            "[101, 126200] loss: 0.339 accuracy: 75 %\n",
            "lr 0.00030656039239730225\n",
            "[102, 126650] loss: 0.330 accuracy: 76 %\n",
            "lr 0.00030613806826878924\n",
            "[102, 127050] loss: 0.339 accuracy: 76 %\n",
            "lr 0.00030576364470264486\n",
            "[102, 127450] loss: 0.327 accuracy: 75 %\n",
            "lr 0.00030539013589861045\n",
            "[103, 127900] loss: 0.334 accuracy: 75 %\n",
            "lr 0.0003049710277523635\n",
            "[103, 128300] loss: 0.332 accuracy: 75 %\n",
            "lr 0.00030459945172098686\n",
            "[103, 128700] loss: 0.319 accuracy: 75 %\n",
            "lr 0.000304228780042592\n",
            "[104, 129150] loss: 0.313 accuracy: 76 %\n",
            "lr 0.0003038128512836093\n",
            "[104, 129550] loss: 0.334 accuracy: 75 %\n",
            "lr 0.00030344409042633893\n",
            "[104, 129950] loss: 0.326 accuracy: 75 %\n",
            "lr 0.00030307622367025305\n",
            "[105, 130400] loss: 0.312 accuracy: 75 %\n",
            "lr 0.00030266343825665856\n",
            "[105, 130800] loss: 0.329 accuracy: 76 %\n",
            "lr 0.00030229746070133015\n",
            "[105, 131200] loss: 0.321 accuracy: 76 %\n",
            "lr 0.00030193236714975844\n",
            "[106, 131650] loss: 0.337 accuracy: 75 %\n",
            "lr 0.00030152268958239106\n",
            "[106, 132050] loss: 0.307 accuracy: 75 %\n",
            "lr 0.0003011594639361542\n",
            "[106, 132450] loss: 0.332 accuracy: 75 %\n",
            "lr 0.00030079711234772146\n",
            "[107, 132900] loss: 0.317 accuracy: 75 %\n",
            "lr 0.00030039050765995795\n",
            "[107, 133300] loss: 0.320 accuracy: 75 %\n",
            "lr 0.0003000300030003\n",
            "[107, 133700] loss: 0.327 accuracy: 75 %\n",
            "lr 0.0002996703626011387\n",
            "[108, 134150] loss: 0.322 accuracy: 75 %\n",
            "lr 0.0002992667963489451\n",
            "[108, 134550] loss: 0.312 accuracy: 75 %\n",
            "lr 0.00029890898221491554\n",
            "[108, 134950] loss: 0.314 accuracy: 75 %\n",
            "lr 0.00029855202268995375\n",
            "[109, 135400] loss: 0.330 accuracy: 75 %\n",
            "lr 0.0002981514609421586\n",
            "[109, 135800] loss: 0.309 accuracy: 76 %\n",
            "lr 0.00029779630732578913\n",
            "[109, 136200] loss: 0.333 accuracy: 75 %\n",
            "lr 0.000297441998810232\n",
            "[110, 136650] loss: 0.318 accuracy: 76 %\n",
            "lr 0.0002970444081390168\n",
            "[110, 137050] loss: 0.327 accuracy: 76 %\n",
            "lr 0.00029669188547693225\n",
            "[110, 137450] loss: 0.314 accuracy: 75 %\n",
            "lr 0.000296340198547933\n",
            "[111, 137900] loss: 0.313 accuracy: 75 %\n",
            "lr 0.0002959455460195324\n",
            "[111, 138300] loss: 0.317 accuracy: 75 %\n",
            "lr 0.0002955956251847473\n",
            "[111, 138700] loss: 0.328 accuracy: 75 %\n",
            "lr 0.0002952465308532625\n",
            "[112, 139150] loss: 0.302 accuracy: 75 %\n",
            "lr 0.0002948547840188707\n",
            "[112, 139550] loss: 0.324 accuracy: 75 %\n",
            "lr 0.0002945074363127669\n",
            "[112, 139950] loss: 0.316 accuracy: 75 %\n",
            "lr 0.00029416090601559053\n",
            "[113, 140400] loss: 0.322 accuracy: 75 %\n",
            "lr 0.0002937720329024677\n",
            "[113, 140800] loss: 0.323 accuracy: 75 %\n",
            "lr 0.0002934272300469483\n",
            "[113, 141200] loss: 0.321 accuracy: 76 %\n",
            "lr 0.0002930832356389215\n",
            "[114, 141650] loss: 0.314 accuracy: 75 %\n",
            "lr 0.0002926972047416947\n",
            "[114, 142050] loss: 0.324 accuracy: 76 %\n",
            "lr 0.00029235491887151\n",
            "[114, 142450] loss: 0.316 accuracy: 76 %\n",
            "lr 0.0002920134326179004\n",
            "[115, 142900] loss: 0.305 accuracy: 76 %\n",
            "lr 0.0002916302128900554\n",
            "[115, 143300] loss: 0.315 accuracy: 76 %\n",
            "lr 0.0002912904165452957\n",
            "[115, 143700] loss: 0.314 accuracy: 75 %\n",
            "lr 0.00029095141111434386\n",
            "[116, 144150] loss: 0.312 accuracy: 75 %\n",
            "lr 0.00029057097195990116\n",
            "[116, 144550] loss: 0.308 accuracy: 75 %\n",
            "lr 0.0002902336380786533\n",
            "[116, 144950] loss: 0.308 accuracy: 76 %\n",
            "lr 0.00028989708653428034\n",
            "[117, 145400] loss: 0.310 accuracy: 76 %\n",
            "lr 0.00028951939779965256\n",
            "[117, 145800] loss: 0.304 accuracy: 76 %\n",
            "lr 0.0002891844997108155\n",
            "[117, 146200] loss: 0.309 accuracy: 75 %\n",
            "lr 0.00028885037550548814\n",
            "[118, 146650] loss: 0.310 accuracy: 75 %\n",
            "lr 0.0002884754074715131\n",
            "[118, 147050] loss: 0.321 accuracy: 75 %\n",
            "lr 0.0002881429188877683\n",
            "[118, 147450] loss: 0.317 accuracy: 76 %\n",
            "lr 0.0002878111958555188\n",
            "[119, 147900] loss: 0.309 accuracy: 76 %\n",
            "lr 0.0002874389192296637\n",
            "[119, 148300] loss: 0.308 accuracy: 76 %\n",
            "lr 0.0002871088142405972\n",
            "[119, 148700] loss: 0.307 accuracy: 76 %\n",
            "lr 0.00028677946659019213\n",
            "[120, 149150] loss: 0.312 accuracy: 76 %\n",
            "lr 0.00028640985249892595\n",
            "[120, 149550] loss: 0.311 accuracy: 76 %\n",
            "lr 0.000286082105564297\n",
            "[120, 149950] loss: 0.312 accuracy: 76 %\n",
            "lr 0.0002857551078725532\n",
            "[121, 150400] loss: 0.294 accuracy: 76 %\n",
            "lr 0.00028538812785388126\n",
            "[121, 150800] loss: 0.299 accuracy: 76 %\n",
            "lr 0.00028506271379703536\n",
            "[121, 151200] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002847380410022779\n",
            "[122, 151650] loss: 0.297 accuracy: 76 %\n",
            "lr 0.00028437366699843596\n",
            "[122, 152050] loss: 0.308 accuracy: 75 %\n",
            "lr 0.00028405056099985794\n",
            "[122, 152450] loss: 0.308 accuracy: 76 %\n",
            "lr 0.0002837281883955171\n",
            "[123, 152900] loss: 0.304 accuracy: 75 %\n",
            "lr 0.00028336639274582036\n",
            "[123, 153300] loss: 0.313 accuracy: 75 %\n",
            "lr 0.0002830455703368242\n",
            "[123, 153700] loss: 0.313 accuracy: 76 %\n",
            "lr 0.00028272547356516825\n",
            "[124, 154150] loss: 0.290 accuracy: 76 %\n",
            "lr 0.00028236622899901174\n",
            "[124, 154550] loss: 0.312 accuracy: 76 %\n",
            "lr 0.0002820476660555634\n",
            "[124, 154950] loss: 0.300 accuracy: 76 %\n",
            "lr 0.00028172982110156357\n",
            "[125, 155400] loss: 0.304 accuracy: 76 %\n",
            "lr 0.00028137310073157\n",
            "[125, 155800] loss: 0.309 accuracy: 76 %\n",
            "lr 0.0002810567734682406\n",
            "[125, 156200] loss: 0.318 accuracy: 76 %\n",
            "lr 0.0002807411566535654\n",
            "[126, 156650] loss: 0.304 accuracy: 76 %\n",
            "lr 0.00028038693396887705\n",
            "[126, 157050] loss: 0.296 accuracy: 76 %\n",
            "lr 0.0002800728189329226\n",
            "[126, 157450] loss: 0.303 accuracy: 75 %\n",
            "lr 0.00027975940691005733\n",
            "[127, 157900] loss: 0.302 accuracy: 76 %\n",
            "lr 0.0002794076557697681\n",
            "[127, 158300] loss: 0.295 accuracy: 76 %\n",
            "lr 0.00027909572983533354\n",
            "[127, 158700] loss: 0.299 accuracy: 76 %\n",
            "lr 0.00027878449958182325\n",
            "[128, 159150] loss: 0.295 accuracy: 76 %\n",
            "lr 0.00027843519420854795\n",
            "[128, 159550] loss: 0.297 accuracy: 76 %\n",
            "lr 0.0002781254345709915\n",
            "[128, 159950] loss: 0.303 accuracy: 76 %\n",
            "lr 0.0002778163633838033\n",
            "[129, 160400] loss: 0.300 accuracy: 76 %\n",
            "lr 0.0002774694783573807\n",
            "[129, 160800] loss: 0.292 accuracy: 76 %\n",
            "lr 0.0002771618625277162\n",
            "[129, 161200] loss: 0.314 accuracy: 76 %\n",
            "lr 0.0002768549280177187\n",
            "[130, 161650] loss: 0.308 accuracy: 76 %\n",
            "lr 0.00027651043826904464\n",
            "[130, 162050] loss: 0.299 accuracy: 76 %\n",
            "lr 0.00027620494406849883\n",
            "[130, 162450] loss: 0.295 accuracy: 76 %\n",
            "lr 0.00027590012415505585\n",
            "[131, 162900] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00027555800496004406\n",
            "[131, 163300] loss: 0.297 accuracy: 76 %\n",
            "lr 0.0002752546105147261\n",
            "[131, 163700] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002749518834204014\n",
            "[132, 164150] loss: 0.287 accuracy: 76 %\n",
            "lr 0.0002746121103940684\n",
            "[132, 164550] loss: 0.291 accuracy: 76 %\n",
            "lr 0.000274310794129749\n",
            "[132, 164950] loss: 0.294 accuracy: 76 %\n",
            "lr 0.0002740101383751199\n",
            "[133, 165400] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002736726874657909\n",
            "[133, 165800] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002733734281027884\n",
            "[133, 166200] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002730748225013654\n",
            "[134, 166650] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002727396699849993\n",
            "[134, 167050] loss: 0.299 accuracy: 76 %\n",
            "lr 0.0002724424465331699\n",
            "[134, 167450] loss: 0.306 accuracy: 76 %\n",
            "lr 0.00027214587018641994\n",
            "[135, 167900] loss: 0.295 accuracy: 76 %\n",
            "lr 0.0002718129926610492\n",
            "[135, 168300] loss: 0.298 accuracy: 76 %\n",
            "lr 0.00027151778441487917\n",
            "[135, 168700] loss: 0.296 accuracy: 76 %\n",
            "lr 0.00027122321670735016\n",
            "[136, 169150] loss: 0.293 accuracy: 76 %\n",
            "lr 0.0002708925910876337\n",
            "[136, 169550] loss: 0.289 accuracy: 76 %\n",
            "lr 0.00027059937762143147\n",
            "[136, 169950] loss: 0.294 accuracy: 76 %\n",
            "lr 0.0002703067982159751\n",
            "[137, 170400] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002699784017278618\n",
            "[137, 170800] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00026968716289104636\n",
            "[137, 171200] loss: 0.297 accuracy: 75 %\n",
            "lr 0.00026939655172413793\n",
            "[138, 171650] loss: 0.302 accuracy: 76 %\n",
            "lr 0.0002690703618996368\n",
            "[138, 172050] loss: 0.290 accuracy: 76 %\n",
            "lr 0.000268781077812122\n",
            "[138, 172450] loss: 0.288 accuracy: 76 %\n",
            "lr 0.0002684924150892737\n",
            "[139, 172900] loss: 0.300 accuracy: 76 %\n",
            "lr 0.0002681684097613301\n",
            "[139, 173300] loss: 0.293 accuracy: 76 %\n",
            "lr 0.0002678810608090008\n",
            "[139, 173700] loss: 0.285 accuracy: 76 %\n",
            "lr 0.0002675943270002676\n",
            "[140, 174150] loss: 0.275 accuracy: 76 %\n",
            "lr 0.00026727248429774154\n",
            "[140, 174550] loss: 0.294 accuracy: 76 %\n",
            "lr 0.0002669870511280203\n",
            "[140, 174950] loss: 0.288 accuracy: 76 %\n",
            "lr 0.0002667022269635951\n",
            "[141, 175400] loss: 0.287 accuracy: 76 %\n",
            "lr 0.0002663825253063399\n",
            "[141, 175800] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00026609898882384245\n",
            "[141, 176200] loss: 0.291 accuracy: 76 %\n",
            "lr 0.00026581605528973947\n",
            "[142, 176650] loss: 0.302 accuracy: 76 %\n",
            "lr 0.00026549847338377806\n",
            "[142, 177050] loss: 0.301 accuracy: 76 %\n",
            "lr 0.0002652168147460549\n",
            "[142, 177450] loss: 0.291 accuracy: 76 %\n",
            "lr 0.00026493575307987813\n",
            "[143, 177900] loss: 0.282 accuracy: 76 %\n",
            "lr 0.0002646202699126753\n",
            "[143, 178300] loss: 0.295 accuracy: 76 %\n",
            "lr 0.0002643404705260375\n",
            "[143, 178700] loss: 0.292 accuracy: 76 %\n",
            "lr 0.0002640612622128334\n",
            "[144, 179150] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00026374785704866147\n",
            "[144, 179550] loss: 0.290 accuracy: 76 %\n",
            "lr 0.00026346989856408903\n",
            "[144, 179950] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00026319252533228054\n",
            "[145, 180400] loss: 0.279 accuracy: 76 %\n",
            "lr 0.0002628811777076761\n",
            "[145, 180800] loss: 0.279 accuracy: 76 %\n",
            "lr 0.00026260504201680677\n",
            "[145, 181200] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00026232948583420777\n",
            "[146, 181650] loss: 0.288 accuracy: 76 %\n",
            "lr 0.0002620201755535176\n",
            "[146, 182050] loss: 0.286 accuracy: 76 %\n",
            "lr 0.00026174584478471405\n",
            "[146, 182450] loss: 0.294 accuracy: 76 %\n",
            "lr 0.0002614720878546215\n",
            "[147, 182900] loss: 0.290 accuracy: 76 %\n",
            "lr 0.00026116479498563595\n",
            "[147, 183300] loss: 0.287 accuracy: 76 %\n",
            "lr 0.00026089225150013044\n",
            "[147, 183700] loss: 0.275 accuracy: 76 %\n",
            "lr 0.00026062027625749283\n",
            "[148, 184150] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002603149811271639\n",
            "[148, 184550] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002600442075152776\n",
            "[148, 184950] loss: 0.276 accuracy: 76 %\n",
            "lr 0.0002597739966229381\n",
            "[149, 185400] loss: 0.288 accuracy: 76 %\n",
            "lr 0.0002594706798131811\n",
            "[149, 185800] loss: 0.293 accuracy: 76 %\n",
            "lr 0.0002592016588906169\n",
            "[149, 186200] loss: 0.299 accuracy: 76 %\n",
            "lr 0.0002589331952356292\n",
            "[150, 186650] loss: 0.283 accuracy: 77 %\n",
            "lr 0.00025863183757920597\n",
            "[150, 187050] loss: 0.292 accuracy: 76 %\n",
            "lr 0.000258364552383413\n",
            "[150, 187450] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002580978190734288\n",
            "[151, 187900] loss: 0.271 accuracy: 76 %\n",
            "lr 0.00025779840164990973\n",
            "[151, 188300] loss: 0.276 accuracy: 76 %\n",
            "lr 0.0002575328354365182\n",
            "[151, 188700] loss: 0.283 accuracy: 76 %\n",
            "lr 0.00025726781579624386\n",
            "[152, 189150] loss: 0.273 accuracy: 76 %\n",
            "lr 0.00025697031992804833\n",
            "[152, 189550] loss: 0.275 accuracy: 76 %\n",
            "lr 0.0002567064561673726\n",
            "[152, 189950] loss: 0.289 accuracy: 76 %\n",
            "lr 0.00025644313373509426\n",
            "[153, 190400] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00025614754098360657\n",
            "[153, 190800] loss: 0.276 accuracy: 76 %\n",
            "lr 0.00025588536335721597\n",
            "[153, 191200] loss: 0.288 accuracy: 76 %\n",
            "lr 0.0002556237218813906\n",
            "[154, 191650] loss: 0.280 accuracy: 76 %\n",
            "lr 0.00025533001404315077\n",
            "[154, 192050] loss: 0.292 accuracy: 76 %\n",
            "lr 0.00025506950644050506\n",
            "[154, 192450] loss: 0.274 accuracy: 76 %\n",
            "lr 0.00025480952987641737\n",
            "[155, 192900] loss: 0.276 accuracy: 76 %\n",
            "lr 0.00025451768897938407\n",
            "[155, 193300] loss: 0.283 accuracy: 76 %\n",
            "lr 0.00025425883549453347\n",
            "[155, 193700] loss: 0.278 accuracy: 76 %\n",
            "lr 0.000254000508001016\n",
            "[156, 194150] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00025371051630090067\n",
            "[156, 194550] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002534533012292485\n",
            "[156, 194950] loss: 0.274 accuracy: 76 %\n",
            "lr 0.000253196607165464\n",
            "[157, 195400] loss: 0.282 accuracy: 76 %\n",
            "lr 0.00025290844714213456\n",
            "[157, 195800] loss: 0.280 accuracy: 76 %\n",
            "lr 0.00025265285497726126\n",
            "[157, 196200] loss: 0.282 accuracy: 76 %\n",
            "lr 0.0002523977788995457\n",
            "[158, 196650] loss: 0.272 accuracy: 76 %\n",
            "lr 0.00025211143325349803\n",
            "[158, 197050] loss: 0.272 accuracy: 76 %\n",
            "lr 0.0002518574486840448\n",
            "[158, 197450] loss: 0.271 accuracy: 76 %\n",
            "lr 0.0002516039753428104\n",
            "[159, 197900] loss: 0.266 accuracy: 76 %\n",
            "lr 0.00025131942699170643\n",
            "[159, 198300] loss: 0.273 accuracy: 76 %\n",
            "lr 0.00025106703489831785\n",
            "[159, 198700] loss: 0.282 accuracy: 76 %\n",
            "lr 0.00025081514923501377\n",
            "[160, 199150] loss: 0.277 accuracy: 76 %\n",
            "lr 0.0002505323813102843\n",
            "[160, 199550] loss: 0.291 accuracy: 76 %\n",
            "lr 0.0002502815667626079\n",
            "[160, 199950] loss: 0.281 accuracy: 76 %\n",
            "lr 0.0002500312539067383\n",
            "[161, 200400] loss: 0.272 accuracy: 76 %\n",
            "lr 0.0002497502497502498\n",
            "[161, 200800] loss: 0.282 accuracy: 76 %\n",
            "lr 0.000249500998003992\n",
            "[161, 201200] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002492522432701894\n",
            "[162, 201650] loss: 0.273 accuracy: 76 %\n",
            "lr 0.0002489729864309722\n",
            "[162, 202050] loss: 0.277 accuracy: 76 %\n",
            "lr 0.0002487252829250093\n",
            "[162, 202450] loss: 0.281 accuracy: 76 %\n",
            "lr 0.0002484780718101628\n",
            "[163, 202900] loss: 0.265 accuracy: 76 %\n",
            "lr 0.0002482005460412013\n",
            "[163, 203300] loss: 0.286 accuracy: 76 %\n",
            "lr 0.0002479543763947433\n",
            "[163, 203700] loss: 0.274 accuracy: 76 %\n",
            "lr 0.00024770869457517957\n",
            "[164, 204150] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00024743288383026105\n",
            "[164, 204550] loss: 0.264 accuracy: 76 %\n",
            "lr 0.0002471882338400692\n",
            "[164, 204950] loss: 0.271 accuracy: 76 %\n",
            "lr 0.0002469440671687863\n",
            "[165, 205400] loss: 0.263 accuracy: 76 %\n",
            "lr 0.00024666995559940796\n",
            "[165, 205800] loss: 0.266 accuracy: 76 %\n",
            "lr 0.0002464268112370626\n",
            "[165, 206200] loss: 0.282 accuracy: 76 %\n",
            "lr 0.00024618414574101424\n",
            "[166, 206650] loss: 0.273 accuracy: 77 %\n",
            "lr 0.00024591171769334814\n",
            "[166, 207050] loss: 0.269 accuracy: 76 %\n",
            "lr 0.00024567006510256724\n",
            "[166, 207450] loss: 0.268 accuracy: 76 %\n",
            "lr 0.00024542888697999754\n",
            "[167, 207900] loss: 0.277 accuracy: 76 %\n",
            "lr 0.00024515812699190976\n",
            "[167, 208300] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002449179524859172\n",
            "[167, 208700] loss: 0.265 accuracy: 76 %\n",
            "lr 0.0002446782481037436\n",
            "[168, 209150] loss: 0.280 accuracy: 76 %\n",
            "lr 0.0002444091409018697\n",
            "[168, 209550] loss: 0.282 accuracy: 76 %\n",
            "lr 0.00024417043096081065\n",
            "[168, 209950] loss: 0.262 accuracy: 76 %\n",
            "lr 0.00024393218685205508\n",
            "[169, 210400] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00024366471734892786\n",
            "[169, 210800] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00024342745861733202\n",
            "[169, 211200] loss: 0.274 accuracy: 77 %\n",
            "lr 0.00024319066147859923\n",
            "[170, 211650] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00024292481476982872\n",
            "[170, 212050] loss: 0.256 accuracy: 76 %\n",
            "lr 0.00024268899405411966\n",
            "[170, 212450] loss: 0.263 accuracy: 77 %\n",
            "lr 0.00024245363074312036\n",
            "[171, 212900] loss: 0.264 accuracy: 76 %\n",
            "lr 0.00024218939210462584\n",
            "[171, 213300] loss: 0.269 accuracy: 76 %\n",
            "lr 0.00024195499637067505\n",
            "[171, 213700] loss: 0.270 accuracy: 76 %\n",
            "lr 0.000241721053903795\n",
            "[172, 214150] loss: 0.265 accuracy: 76 %\n",
            "lr 0.00024145840878908606\n",
            "[172, 214550] loss: 0.275 accuracy: 76 %\n",
            "lr 0.00024122542515981184\n",
            "[172, 214950] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002409928907097241\n",
            "[173, 215400] loss: 0.270 accuracy: 76 %\n",
            "lr 0.0002407318247472316\n",
            "[173, 215800] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00024050024050024048\n",
            "[173, 216200] loss: 0.267 accuracy: 77 %\n",
            "lr 0.00024026910139356074\n",
            "[174, 216650] loss: 0.279 accuracy: 76 %\n",
            "lr 0.00024000960038401536\n",
            "[174, 217050] loss: 0.273 accuracy: 76 %\n",
            "lr 0.00023977940294928662\n",
            "[174, 217450] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00023954964666427118\n",
            "[175, 217900] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00023929169657812874\n",
            "[175, 218300] loss: 0.278 accuracy: 76 %\n",
            "lr 0.00023906287353573993\n",
            "[175, 218700] loss: 0.268 accuracy: 76 %\n",
            "lr 0.00023883448770002386\n",
            "[176, 219150] loss: 0.263 accuracy: 76 %\n",
            "lr 0.0002385780746749374\n",
            "[176, 219550] loss: 0.263 accuracy: 76 %\n",
            "lr 0.00023835061375283043\n",
            "[176, 219950] loss: 0.262 accuracy: 76 %\n",
            "lr 0.00023812358614120726\n",
            "[177, 220400] loss: 0.265 accuracy: 76 %\n",
            "lr 0.00023786869647954325\n",
            "[177, 220800] loss: 0.260 accuracy: 76 %\n",
            "lr 0.0002376425855513308\n",
            "[177, 221200] loss: 0.262 accuracy: 76 %\n",
            "lr 0.00023741690408357076\n",
            "[178, 221650] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00023716352424997037\n",
            "[178, 222050] loss: 0.272 accuracy: 76 %\n",
            "lr 0.00023693875133278046\n",
            "[178, 222450] loss: 0.259 accuracy: 76 %\n",
            "lr 0.0002367144040714877\n",
            "[179, 222900] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00023646252069047056\n",
            "[179, 223300] loss: 0.266 accuracy: 77 %\n",
            "lr 0.00023623907394283012\n",
            "[179, 223700] loss: 0.266 accuracy: 76 %\n",
            "lr 0.0002360160490913382\n",
            "[180, 224150] loss: 0.266 accuracy: 76 %\n",
            "lr 0.0002357656489449487\n",
            "[180, 224550] loss: 0.263 accuracy: 76 %\n",
            "lr 0.0002355435166647038\n",
            "[180, 224950] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00023532180256500765\n",
            "[181, 225400] loss: 0.267 accuracy: 76 %\n",
            "lr 0.0002350728725905031\n",
            "[181, 225800] loss: 0.253 accuracy: 76 %\n",
            "lr 0.00023485204321277596\n",
            "[181, 226200] loss: 0.264 accuracy: 76 %\n",
            "lr 0.0002346316283435007\n",
            "[182, 226650] loss: 0.266 accuracy: 76 %\n",
            "lr 0.0002343841556310793\n",
            "[182, 227050] loss: 0.256 accuracy: 76 %\n",
            "lr 0.00023416461772626156\n",
            "[182, 227450] loss: 0.261 accuracy: 76 %\n",
            "lr 0.00023394549070066676\n",
            "[183, 227900] loss: 0.260 accuracy: 76 %\n",
            "lr 0.00023369946249123628\n",
            "[183, 228300] loss: 0.254 accuracy: 76 %\n",
            "lr 0.00023348120476301656\n",
            "[183, 228700] loss: 0.273 accuracy: 76 %\n",
            "lr 0.00023326335432703518\n",
            "[184, 229150] loss: 0.269 accuracy: 76 %\n",
            "lr 0.0002330187580100198\n",
            "[184, 229550] loss: 0.260 accuracy: 76 %\n",
            "lr 0.0002328017692934466\n",
            "[184, 229950] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00023258518432375856\n",
            "[185, 230400] loss: 0.247 accuracy: 77 %\n",
            "lr 0.00023234200743494423\n",
            "[185, 230800] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00023212627669452182\n",
            "[185, 231200] loss: 0.265 accuracy: 76 %\n",
            "lr 0.00023191094619666046\n",
            "[186, 231650] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00023166917641607787\n",
            "[186, 232050] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002314546927438954\n",
            "[186, 232450] loss: 0.260 accuracy: 76 %\n",
            "lr 0.00023124060585038732\n",
            "[187, 232900] loss: 0.260 accuracy: 76 %\n",
            "lr 0.00023100023100023096\n",
            "[187, 233300] loss: 0.262 accuracy: 76 %\n",
            "lr 0.00023078698361412415\n",
            "[187, 233700] loss: 0.264 accuracy: 76 %\n",
            "lr 0.00023057412958266084\n",
            "[188, 234150] loss: 0.257 accuracy: 77 %\n",
            "lr 0.00023033513762524475\n",
            "[188, 234550] loss: 0.268 accuracy: 77 %\n",
            "lr 0.00023012311586698882\n",
            "[188, 234950] loss: 0.254 accuracy: 76 %\n",
            "lr 0.00022991148407862968\n",
            "[189, 235400] loss: 0.254 accuracy: 76 %\n",
            "lr 0.00022967386311437759\n",
            "[189, 235800] loss: 0.253 accuracy: 77 %\n",
            "lr 0.00022946305644791186\n",
            "[189, 236200] loss: 0.264 accuracy: 76 %\n",
            "lr 0.00022925263640531865\n",
            "[190, 236650] loss: 0.256 accuracy: 76 %\n",
            "lr 0.00022901637467078896\n",
            "[190, 237050] loss: 0.260 accuracy: 76 %\n",
            "lr 0.00022880677268047135\n",
            "[190, 237450] loss: 0.257 accuracy: 76 %\n",
            "lr 0.00022859755400617213\n",
            "[191, 237900] loss: 0.261 accuracy: 76 %\n",
            "lr 0.00022836263987211696\n",
            "[191, 238300] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00022815423226100844\n",
            "[191, 238700] loss: 0.253 accuracy: 76 %\n",
            "lr 0.0002279462046956918\n",
            "[192, 239150] loss: 0.264 accuracy: 76 %\n",
            "lr 0.00022771262666514854\n",
            "[192, 239550] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00022750540325332727\n",
            "[192, 239950] loss: 0.253 accuracy: 76 %\n",
            "lr 0.00022729855665416527\n",
            "[193, 240400] loss: 0.252 accuracy: 76 %\n",
            "lr 0.0002270663033605813\n",
            "[193, 240800] loss: 0.255 accuracy: 76 %\n",
            "lr 0.00022686025408348456\n",
            "[193, 241200] loss: 0.254 accuracy: 77 %\n",
            "lr 0.0002266545784224841\n",
            "[194, 241650] loss: 0.243 accuracy: 76 %\n",
            "lr 0.00022642363862787276\n",
            "[194, 242050] loss: 0.258 accuracy: 76 %\n",
            "lr 0.000226218753534668\n",
            "[194, 242450] loss: 0.248 accuracy: 76 %\n",
            "lr 0.00022601423889705052\n",
            "[195, 242900] loss: 0.258 accuracy: 76 %\n",
            "lr 0.00022578460149017836\n",
            "[195, 243300] loss: 0.270 accuracy: 76 %\n",
            "lr 0.0002255808707421611\n",
            "[195, 243700] loss: 0.250 accuracy: 77 %\n",
            "lr 0.00022537750732476897\n",
            "[196, 244150] loss: 0.256 accuracy: 76 %\n",
            "lr 0.0002251491613193741\n",
            "[196, 244550] loss: 0.251 accuracy: 77 %\n",
            "lr 0.00022494657518839275\n",
            "[196, 244950] loss: 0.251 accuracy: 76 %\n",
            "lr 0.00022474435329812337\n",
            "[197, 245400] loss: 0.254 accuracy: 77 %\n",
            "lr 0.00022451728783116296\n",
            "[197, 245800] loss: 0.246 accuracy: 77 %\n",
            "lr 0.00022431583669807088\n",
            "[197, 246200] loss: 0.261 accuracy: 76 %\n",
            "lr 0.00022411474675033618\n",
            "[198, 246650] loss: 0.257 accuracy: 76 %\n",
            "lr 0.0002238889510802642\n",
            "[198, 247050] loss: 0.248 accuracy: 77 %\n",
            "lr 0.00022368862543339669\n",
            "[198, 247450] loss: 0.259 accuracy: 76 %\n",
            "lr 0.00022348865795060897\n",
            "[199, 247900] loss: 0.268 accuracy: 76 %\n",
            "lr 0.00022326412145568208\n",
            "[199, 248300] loss: 0.253 accuracy: 77 %\n",
            "lr 0.0002230649118893598\n",
            "[199, 248700] loss: 0.244 accuracy: 77 %\n",
            "lr 0.00022286605749944285\n",
            "[200, 249150] loss: 0.254 accuracy: 76 %\n",
            "lr 0.00022264276967605477\n",
            "[200, 249550] loss: 0.243 accuracy: 76 %\n",
            "lr 0.00022244466688911134\n",
            "[200, 249950] loss: 0.254 accuracy: 77 %\n",
            "lr 0.000222246916324036\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "trainer.train(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIW4yMXrdmyb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "337ba3f2b64b483eb15a7c2769b27e17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4adfa694ca1040899aed6a26be808540": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5354924b45f5478d9e49d04fdc6d7a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a0cdd5f0a4e44a6b73b3e578522317a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6909966f928040fa979d882985a777b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0616d758ae84331af5376c7007e455a",
            "placeholder": "",
            "style": "IPY_MODEL_5a0cdd5f0a4e44a6b73b3e578522317a",
            "value": "100%"
          }
        },
        "6a7c69a34ba94d419000c814ca20be6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6909966f928040fa979d882985a777b3",
              "IPY_MODEL_c6b4ae84acc845fd94f92476e79dbabc",
              "IPY_MODEL_78d3b5af789e4096859d7f4c307cbc69"
            ],
            "layout": "IPY_MODEL_4adfa694ca1040899aed6a26be808540"
          }
        },
        "78d3b5af789e4096859d7f4c307cbc69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6c40a11513947ebbcbfe3f3e6cb82d0",
            "placeholder": "",
            "style": "IPY_MODEL_f13b6d03be264bdfbd13b4013d64514d",
            "value": " 170498071/170498071 [00:02&lt;00:00, 88713983.69it/s]"
          }
        },
        "c6b4ae84acc845fd94f92476e79dbabc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337ba3f2b64b483eb15a7c2769b27e17",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5354924b45f5478d9e49d04fdc6d7a92",
            "value": 170498071
          }
        },
        "c6c40a11513947ebbcbfe3f3e6cb82d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0616d758ae84331af5376c7007e455a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f13b6d03be264bdfbd13b4013d64514d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}