{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nM4iedsjAt6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
        "from torch.utils import model_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI7FfpQVw53P"
      },
      "outputs": [],
      "source": [
        "train_policy = transforms.Compose([\n",
        "                    transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], # specific to CIFAR-10, from https://towardsdatascience.com/data-augmentations-in-torchvision-5d56d70c372e\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "\n",
        "test_policy = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "                ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "07524fa6ad4f479c9078b8b5b8f00b72",
            "91cd76a4cd1242b69da3431b3cd85fea",
            "b17ccff517aa43a3893a9f1245de4a5d",
            "b5309cae064b4f3783e3ab4759315716",
            "bef19993c0014b5394f5924659d97043",
            "90a3a02cba8c43bf9e9e25e8d200729b",
            "a532249d2c204791bef03c6d21548601",
            "ff059c7d378040a599c6cc2c14db1ce8",
            "e8917e7a32474d829b3711c31b9179b9",
            "307c6d6782eb4b1fb250c2e5595c65f3",
            "e994c2fa55e84eb19c77b2e911c532a2"
          ]
        },
        "id": "TRtP734ljNJh",
        "outputId": "458873e0-735c-4a14-ef1a-50d72f56c6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07524fa6ad4f479c9078b8b5b8f00b72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_policy)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=40, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_policy)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=40, shuffle=False, num_workers=2)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWdTEY4D3k92"
      },
      "source": [
        "# Defining the Model\n",
        "## Credit: Took lots of code from https://github.com/pytorch/vision/blob/a9a8220e0bcb4ce66a733f8c03a1c2f6c68d22cb/torchvision/models/resnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijLWPa9e3jTx"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "        #                                dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5IX4Yz-jOnm"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "model = ResNet(BasicBlock, [5, 3, 3])\n",
        "# model = ResNet(BasicBlock, [2, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcahYVJR1oI8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB01cvSG2nnW",
        "outputId": "8e1dfaf0-5a05-4f33-9bb9-387357714b21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4737832"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDYs76at2xCY"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, iteration_count,lr=5e-4,lr_decay=5e-6):\n",
        "    lr = lr / (1.0 + lr_decay * iteration_count)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b__ZT7kX_Se"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self):\n",
        "    self.loss_t = []\n",
        "    self.steps_t = []\n",
        "    self.acc_t = []\n",
        "    self.global_step = 0\n",
        "    self.epoch = 0\n",
        "\n",
        "  def train(self, max_epoch):\n",
        "    for self.epoch in range(max_epoch):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "        self.global_step += 1\n",
        "        adjust_learning_rate(optimizer, self.global_step)\n",
        "        model.cuda()\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i%400 == 399:\n",
        "          self.loss_t.append(running_loss)\n",
        "          self.steps_t.append(self.global_step)\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "            for data in testloader:\n",
        "              images, labels = data\n",
        "              images, labels = images.to(device), labels.to(device)\n",
        "              outputs = model(images)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "              accu = 100 * correct / total\n",
        "          print('[%d, %5d] loss: %.3f accuracy: %d %%' % (self.epoch + 1, self.global_step, running_loss / 400, (accu)))\n",
        "          print('lr',optimizer.param_groups[0]['lr'])\n",
        "          self.acc_t.append(accu)\n",
        "          running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh_Q71XkYAYx"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgocteh-YI3L",
        "outputId": "450fc423-84ad-495d-d716-832fd9fb8c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   400] loss: 2.550 accuracy: 32 %\n",
            "lr 0.000499001996007984\n",
            "[1,   800] loss: 1.922 accuracy: 38 %\n",
            "lr 0.00049800796812749\n",
            "[1,  1200] loss: 1.814 accuracy: 42 %\n",
            "lr 0.0004970178926441352\n",
            "[2,  1650] loss: 1.723 accuracy: 45 %\n",
            "lr 0.0004959087527894867\n",
            "[2,  2050] loss: 1.673 accuracy: 47 %\n",
            "lr 0.0004949269982677555\n",
            "[2,  2450] loss: 1.653 accuracy: 49 %\n",
            "lr 0.0004939491232403062\n",
            "[3,  2900] loss: 1.583 accuracy: 51 %\n",
            "lr 0.0004928536224741252\n",
            "[3,  3300] loss: 1.544 accuracy: 52 %\n",
            "lr 0.0004918839153959666\n",
            "[3,  3700] loss: 1.519 accuracy: 53 %\n",
            "lr 0.0004909180166912126\n",
            "[4,  4150] loss: 1.439 accuracy: 55 %\n",
            "lr 0.0004898359049718344\n",
            "[4,  4550] loss: 1.438 accuracy: 55 %\n",
            "lr 0.0004888780249327792\n",
            "[4,  4950] loss: 1.414 accuracy: 57 %\n",
            "lr 0.00048792388387411563\n",
            "[5,  5400] loss: 1.361 accuracy: 57 %\n",
            "lr 0.00048685491723466414\n",
            "[5,  5800] loss: 1.350 accuracy: 59 %\n",
            "lr 0.0004859086491739553\n",
            "[5,  6200] loss: 1.321 accuracy: 59 %\n",
            "lr 0.0004849660523763337\n",
            "[6,  6650] loss: 1.300 accuracy: 61 %\n",
            "lr 0.0004839099927413501\n",
            "[6,  7050] loss: 1.255 accuracy: 60 %\n",
            "lr 0.0004829751267809708\n",
            "[6,  7450] loss: 1.255 accuracy: 62 %\n",
            "lr 0.00048204386599180526\n",
            "[7,  7900] loss: 1.226 accuracy: 62 %\n",
            "lr 0.00048100048100048096\n",
            "[7,  8300] loss: 1.199 accuracy: 63 %\n",
            "lr 0.00048007681228996637\n",
            "[7,  8700] loss: 1.208 accuracy: 64 %\n",
            "lr 0.00047915668423574505\n",
            "[8,  9150] loss: 1.171 accuracy: 64 %\n",
            "lr 0.00047812574707147985\n",
            "[8,  9550] loss: 1.158 accuracy: 64 %\n",
            "lr 0.00047721307563827254\n",
            "[8,  9950] loss: 1.167 accuracy: 65 %\n",
            "lr 0.00047630388187663733\n",
            "[9, 10400] loss: 1.118 accuracy: 65 %\n",
            "lr 0.0004752851711026616\n",
            "[9, 10800] loss: 1.124 accuracy: 66 %\n",
            "lr 0.0004743833017077799\n",
            "[9, 11200] loss: 1.106 accuracy: 66 %\n",
            "lr 0.0004734848484848485\n",
            "[10, 11650] loss: 1.075 accuracy: 66 %\n",
            "lr 0.00047247814788566034\n",
            "[10, 12050] loss: 1.084 accuracy: 67 %\n",
            "lr 0.0004715868898844613\n",
            "[10, 12450] loss: 1.072 accuracy: 66 %\n",
            "lr 0.00047069898799717585\n",
            "[11, 12900] loss: 1.040 accuracy: 67 %\n",
            "lr 0.0004697040864255519\n",
            "[11, 13300] loss: 1.026 accuracy: 67 %\n",
            "lr 0.00046882325363338024\n",
            "[11, 13700] loss: 1.030 accuracy: 67 %\n",
            "lr 0.0004679457182966776\n",
            "[12, 14150] loss: 1.004 accuracy: 68 %\n",
            "lr 0.0004669624095260331\n",
            "[12, 14550] loss: 1.004 accuracy: 68 %\n",
            "lr 0.0004660918200885574\n",
            "[12, 14950] loss: 1.009 accuracy: 68 %\n",
            "lr 0.00046522447080716444\n",
            "[13, 15400] loss: 0.966 accuracy: 68 %\n",
            "lr 0.00046425255338904364\n",
            "[13, 15800] loss: 0.967 accuracy: 69 %\n",
            "lr 0.0004633920296570899\n",
            "[13, 16200] loss: 0.965 accuracy: 69 %\n",
            "lr 0.00046253469010175765\n",
            "[14, 16650] loss: 0.942 accuracy: 69 %\n",
            "lr 0.0004615739672282483\n",
            "[14, 17050] loss: 0.951 accuracy: 69 %\n",
            "lr 0.00046072333563695\n",
            "[14, 17450] loss: 0.938 accuracy: 69 %\n",
            "lr 0.0004598758335249483\n",
            "[15, 17900] loss: 0.907 accuracy: 69 %\n",
            "lr 0.00045892611289582384\n",
            "[15, 18300] loss: 0.922 accuracy: 70 %\n",
            "lr 0.00045808520384791576\n",
            "[15, 18700] loss: 0.912 accuracy: 70 %\n",
            "lr 0.0004572473708276178\n",
            "[16, 19150] loss: 0.893 accuracy: 70 %\n",
            "lr 0.0004563084645220169\n",
            "[16, 19550] loss: 0.896 accuracy: 69 %\n",
            "lr 0.0004554771122751082\n",
            "[16, 19950] loss: 0.884 accuracy: 70 %\n",
            "lr 0.0004546487838145033\n",
            "[17, 20400] loss: 0.853 accuracy: 70 %\n",
            "lr 0.00045372050816696913\n",
            "[17, 20800] loss: 0.858 accuracy: 70 %\n",
            "lr 0.00045289855072463763\n",
            "[17, 21200] loss: 0.872 accuracy: 70 %\n",
            "lr 0.0004520795660036166\n",
            "[18, 21650] loss: 0.826 accuracy: 70 %\n",
            "lr 0.0004511617414843222\n",
            "[18, 22050] loss: 0.825 accuracy: 70 %\n",
            "lr 0.0004503490204908805\n",
            "[18, 22450] loss: 0.827 accuracy: 71 %\n",
            "lr 0.0004495392222971455\n",
            "[19, 22900] loss: 0.814 accuracy: 70 %\n",
            "lr 0.00044863167339614175\n",
            "[19, 23300] loss: 0.813 accuracy: 71 %\n",
            "lr 0.0004478280340349306\n",
            "[19, 23700] loss: 0.827 accuracy: 71 %\n",
            "lr 0.0004470272686633885\n",
            "[20, 24150] loss: 0.771 accuracy: 71 %\n",
            "lr 0.00044612982377871963\n",
            "[20, 24550] loss: 0.806 accuracy: 70 %\n",
            "lr 0.00044533511467379205\n",
            "[20, 24950] loss: 0.782 accuracy: 71 %\n",
            "lr 0.00044454323182929544\n",
            "[21, 25400] loss: 0.770 accuracy: 71 %\n",
            "lr 0.00044365572315882877\n",
            "[21, 25800] loss: 0.762 accuracy: 71 %\n",
            "lr 0.0004428697962798937\n",
            "[21, 26200] loss: 0.771 accuracy: 71 %\n",
            "lr 0.00044208664898320074\n",
            "[22, 26650] loss: 0.743 accuracy: 71 %\n",
            "lr 0.00044120891242003087\n",
            "[22, 27050] loss: 0.758 accuracy: 71 %\n",
            "lr 0.0004404316229905307\n",
            "[22, 27450] loss: 0.768 accuracy: 71 %\n",
            "lr 0.0004396570674873598\n",
            "[23, 27900] loss: 0.711 accuracy: 72 %\n",
            "lr 0.00043878894251864854\n",
            "[23, 28300] loss: 0.724 accuracy: 71 %\n",
            "lr 0.0004380201489268507\n",
            "[23, 28700] loss: 0.746 accuracy: 72 %\n",
            "lr 0.00043725404459991256\n",
            "[24, 29150] loss: 0.720 accuracy: 72 %\n",
            "lr 0.0004363953742090334\n",
            "[24, 29550] loss: 0.704 accuracy: 72 %\n",
            "lr 0.00043563493792202136\n",
            "[24, 29950] loss: 0.719 accuracy: 71 %\n",
            "lr 0.0004348771472059143\n",
            "[25, 30400] loss: 0.679 accuracy: 72 %\n",
            "lr 0.00043402777777777775\n",
            "[25, 30800] loss: 0.696 accuracy: 72 %\n",
            "lr 0.00043327556325823227\n",
            "[25, 31200] loss: 0.708 accuracy: 72 %\n",
            "lr 0.0004325259515570935\n",
            "[26, 31650] loss: 0.666 accuracy: 72 %\n",
            "lr 0.0004316857327865314\n",
            "[26, 32050] loss: 0.690 accuracy: 72 %\n",
            "lr 0.00043094160741219563\n",
            "[26, 32450] loss: 0.689 accuracy: 72 %\n",
            "lr 0.00043020004302000433\n",
            "[27, 32900] loss: 0.646 accuracy: 73 %\n",
            "lr 0.0004293688278231\n",
            "[27, 33300] loss: 0.660 accuracy: 72 %\n",
            "lr 0.0004286326618088298\n",
            "[27, 33700] loss: 0.678 accuracy: 72 %\n",
            "lr 0.00042789901583226354\n",
            "[28, 34150] loss: 0.641 accuracy: 72 %\n",
            "lr 0.0004270766602605168\n",
            "[28, 34550] loss: 0.650 accuracy: 73 %\n",
            "lr 0.0004263483265828182\n",
            "[28, 34950] loss: 0.663 accuracy: 72 %\n",
            "lr 0.00042562247286656737\n",
            "[29, 35400] loss: 0.623 accuracy: 73 %\n",
            "lr 0.0004248088360237893\n",
            "[29, 35800] loss: 0.625 accuracy: 72 %\n",
            "lr 0.00042408821034775233\n",
            "[29, 36200] loss: 0.642 accuracy: 72 %\n",
            "lr 0.0004233700254022015\n",
            "[30, 36650] loss: 0.612 accuracy: 72 %\n",
            "lr 0.00042256496936403967\n",
            "[30, 37050] loss: 0.618 accuracy: 73 %\n",
            "lr 0.0004218519299725797\n",
            "[30, 37450] loss: 0.626 accuracy: 73 %\n",
            "lr 0.00042114129290376915\n",
            "[31, 37900] loss: 0.604 accuracy: 73 %\n",
            "lr 0.0004203446826397646\n",
            "[31, 38300] loss: 0.612 accuracy: 73 %\n",
            "lr 0.000419639110365086\n",
            "[31, 38700] loss: 0.613 accuracy: 72 %\n",
            "lr 0.00041893590280687055\n",
            "[32, 39150] loss: 0.586 accuracy: 73 %\n",
            "lr 0.000418147606104955\n",
            "[32, 39550] loss: 0.612 accuracy: 72 %\n",
            "lr 0.0004174493842621582\n",
            "[32, 39950] loss: 0.603 accuracy: 73 %\n",
            "lr 0.0004167534903104813\n",
            "[33, 40400] loss: 0.572 accuracy: 73 %\n",
            "lr 0.00041597337770382697\n",
            "[33, 40800] loss: 0.585 accuracy: 73 %\n",
            "lr 0.0004152823920265781\n",
            "[33, 41200] loss: 0.594 accuracy: 73 %\n",
            "lr 0.00041459369817578774\n",
            "[34, 41650] loss: 0.558 accuracy: 72 %\n",
            "lr 0.0004138216428719222\n",
            "[34, 42050] loss: 0.563 accuracy: 72 %\n",
            "lr 0.0004131377814501136\n",
            "[34, 42450] loss: 0.603 accuracy: 73 %\n",
            "lr 0.00041245617653124357\n",
            "[35, 42900] loss: 0.557 accuracy: 73 %\n",
            "lr 0.0004116920543433511\n",
            "[35, 43300] loss: 0.572 accuracy: 73 %\n",
            "lr 0.00041101520756267986\n",
            "[35, 43700] loss: 0.566 accuracy: 73 %\n",
            "lr 0.00041034058268362735\n",
            "[36, 44150] loss: 0.548 accuracy: 73 %\n",
            "lr 0.0004095842719639566\n",
            "[36, 44550] loss: 0.552 accuracy: 73 %\n",
            "lr 0.0004089143324473523\n",
            "[36, 44950] loss: 0.546 accuracy: 73 %\n",
            "lr 0.0004082465809348847\n",
            "[37, 45400] loss: 0.537 accuracy: 73 %\n",
            "lr 0.00040749796251018743\n",
            "[37, 45800] loss: 0.558 accuracy: 73 %\n",
            "lr 0.0004068348250610252\n",
            "[37, 46200] loss: 0.568 accuracy: 73 %\n",
            "lr 0.00040617384240454913\n",
            "[38, 46650] loss: 0.526 accuracy: 73 %\n",
            "lr 0.00040543279951348065\n",
            "[38, 47050] loss: 0.533 accuracy: 73 %\n",
            "lr 0.0004047763610605141\n",
            "[38, 47450] loss: 0.543 accuracy: 73 %\n",
            "lr 0.000404122044857547\n",
            "[39, 47900] loss: 0.512 accuracy: 74 %\n",
            "lr 0.0004033884630899556\n",
            "[39, 48300] loss: 0.534 accuracy: 74 %\n",
            "lr 0.0004027386226339106\n",
            "[39, 48700] loss: 0.526 accuracy: 73 %\n",
            "lr 0.0004020908725371934\n",
            "[40, 49150] loss: 0.523 accuracy: 73 %\n",
            "lr 0.00040136463977523575\n",
            "[40, 49550] loss: 0.518 accuracy: 73 %\n",
            "lr 0.00040072129833700667\n",
            "[40, 49950] loss: 0.525 accuracy: 74 %\n",
            "lr 0.0004000800160032006\n",
            "[41, 50400] loss: 0.496 accuracy: 73 %\n",
            "lr 0.00039936102236421724\n",
            "[41, 50800] loss: 0.509 accuracy: 73 %\n",
            "lr 0.00039872408293460925\n",
            "[41, 51200] loss: 0.507 accuracy: 74 %\n",
            "lr 0.0003980891719745223\n",
            "[42, 51650] loss: 0.491 accuracy: 73 %\n",
            "lr 0.00039737730975561295\n",
            "[42, 52050] loss: 0.510 accuracy: 74 %\n",
            "lr 0.00039674667724657806\n",
            "[42, 52450] loss: 0.494 accuracy: 73 %\n",
            "lr 0.0003961180431768667\n",
            "[43, 52900] loss: 0.486 accuracy: 73 %\n",
            "lr 0.00039541320680110717\n",
            "[43, 53300] loss: 0.498 accuracy: 73 %\n",
            "lr 0.0003947887879984209\n",
            "[43, 53700] loss: 0.492 accuracy: 74 %\n",
            "lr 0.0003941663381947182\n",
            "[44, 54150] loss: 0.468 accuracy: 73 %\n",
            "lr 0.0003934684241589612\n",
            "[44, 54550] loss: 0.481 accuracy: 74 %\n",
            "lr 0.0003928501276762915\n",
            "[44, 54950] loss: 0.510 accuracy: 73 %\n",
            "lr 0.0003922337713277113\n",
            "[45, 55400] loss: 0.487 accuracy: 73 %\n",
            "lr 0.0003915426781519185\n",
            "[45, 55800] loss: 0.485 accuracy: 74 %\n",
            "lr 0.0003909304143862393\n",
            "[45, 56200] loss: 0.493 accuracy: 73 %\n",
            "lr 0.00039032006245120994\n",
            "[46, 56650] loss: 0.474 accuracy: 74 %\n",
            "lr 0.00038963569062926167\n",
            "[46, 57050] loss: 0.481 accuracy: 74 %\n",
            "lr 0.0003890293717175647\n",
            "[46, 57450] loss: 0.490 accuracy: 74 %\n",
            "lr 0.0003884249368809478\n",
            "[47, 57900] loss: 0.463 accuracy: 74 %\n",
            "lr 0.00038774718883288094\n",
            "[47, 58300] loss: 0.473 accuracy: 74 %\n",
            "lr 0.00038714672861014324\n",
            "[47, 58700] loss: 0.475 accuracy: 74 %\n",
            "lr 0.00038654812524159255\n",
            "[48, 59150] loss: 0.464 accuracy: 74 %\n",
            "lr 0.00038587690526721975\n",
            "[48, 59550] loss: 0.454 accuracy: 74 %\n",
            "lr 0.00038528221922558276\n",
            "[48, 59950] loss: 0.463 accuracy: 74 %\n",
            "lr 0.0003846893633391037\n",
            "[49, 60400] loss: 0.451 accuracy: 74 %\n",
            "lr 0.00038402457757296467\n",
            "[49, 60800] loss: 0.472 accuracy: 74 %\n",
            "lr 0.0003834355828220859\n",
            "[49, 61200] loss: 0.459 accuracy: 74 %\n",
            "lr 0.00038284839203675346\n",
            "[50, 61650] loss: 0.453 accuracy: 73 %\n",
            "lr 0.00038218994840435696\n",
            "[50, 62050] loss: 0.445 accuracy: 74 %\n",
            "lr 0.0003816065636328945\n",
            "[50, 62450] loss: 0.462 accuracy: 74 %\n",
            "lr 0.0003810249571346923\n",
            "[51, 62900] loss: 0.451 accuracy: 74 %\n",
            "lr 0.0003803727653100038\n",
            "[51, 63300] loss: 0.450 accuracy: 74 %\n",
            "lr 0.00037979491074819596\n",
            "[51, 63700] loss: 0.465 accuracy: 74 %\n",
            "lr 0.0003792188092529389\n",
            "[52, 64150] loss: 0.433 accuracy: 74 %\n",
            "lr 0.0003785727806170736\n",
            "[52, 64550] loss: 0.453 accuracy: 74 %\n",
            "lr 0.000378000378000378\n",
            "[52, 64950] loss: 0.453 accuracy: 74 %\n",
            "lr 0.0003774297037176826\n",
            "[53, 65400] loss: 0.439 accuracy: 74 %\n",
            "lr 0.00037678975131876413\n",
            "[53, 65800] loss: 0.448 accuracy: 74 %\n",
            "lr 0.00037622272385252073\n",
            "[53, 66200] loss: 0.445 accuracy: 73 %\n",
            "lr 0.0003756574004507889\n",
            "[54, 66650] loss: 0.426 accuracy: 74 %\n",
            "lr 0.0003750234389649353\n",
            "[54, 67050] loss: 0.443 accuracy: 74 %\n",
            "lr 0.00037446171129002057\n",
            "[54, 67450] loss: 0.432 accuracy: 74 %\n",
            "lr 0.0003739016638624042\n",
            "[55, 67900] loss: 0.427 accuracy: 74 %\n",
            "lr 0.00037327360955580435\n",
            "[55, 68300] loss: 0.421 accuracy: 74 %\n",
            "lr 0.00037271710771524417\n",
            "[55, 68700] loss: 0.435 accuracy: 74 %\n",
            "lr 0.00037216226274655747\n",
            "[56, 69150] loss: 0.414 accuracy: 74 %\n",
            "lr 0.000371540033438603\n",
            "[56, 69550] loss: 0.437 accuracy: 74 %\n",
            "lr 0.00037098868484511225\n",
            "[56, 69950] loss: 0.417 accuracy: 74 %\n",
            "lr 0.0003704389701796629\n",
            "[57, 70400] loss: 0.406 accuracy: 74 %\n",
            "lr 0.0003698224852071006\n",
            "[57, 70800] loss: 0.425 accuracy: 74 %\n",
            "lr 0.0003692762186115214\n",
            "[57, 71200] loss: 0.429 accuracy: 74 %\n",
            "lr 0.0003687315634218289\n",
            "[58, 71650] loss: 0.413 accuracy: 74 %\n",
            "lr 0.0003681207436039021\n",
            "[58, 72050] loss: 0.421 accuracy: 74 %\n",
            "lr 0.00036757948906451023\n",
            "[58, 72450] loss: 0.418 accuracy: 74 %\n",
            "lr 0.0003670398238208846\n",
            "[59, 72900] loss: 0.417 accuracy: 74 %\n",
            "lr 0.00036643459142543056\n",
            "[59, 73300] loss: 0.415 accuracy: 74 %\n",
            "lr 0.00036589828027808267\n",
            "[59, 73700] loss: 0.419 accuracy: 75 %\n",
            "lr 0.00036536353671903543\n",
            "[60, 74150] loss: 0.421 accuracy: 74 %\n",
            "lr 0.0003647638154295094\n",
            "[60, 74550] loss: 0.411 accuracy: 74 %\n",
            "lr 0.000364232380258605\n",
            "[60, 74950] loss: 0.413 accuracy: 74 %\n",
            "lr 0.0003637024913620658\n",
            "[61, 75400] loss: 0.410 accuracy: 75 %\n",
            "lr 0.00036310820624546115\n",
            "[61, 75800] loss: 0.407 accuracy: 74 %\n",
            "lr 0.00036258158085569254\n",
            "[61, 76200] loss: 0.410 accuracy: 74 %\n",
            "lr 0.00036205648081100655\n",
            "[62, 76650] loss: 0.402 accuracy: 74 %\n",
            "lr 0.0003614675582866438\n",
            "[62, 77050] loss: 0.404 accuracy: 74 %\n",
            "lr 0.00036094567767550983\n",
            "[62, 77450] loss: 0.408 accuracy: 74 %\n",
            "lr 0.00036042530185619026\n",
            "[63, 77900] loss: 0.385 accuracy: 74 %\n",
            "lr 0.0003598416696653473\n",
            "[63, 78300] loss: 0.391 accuracy: 74 %\n",
            "lr 0.00035932446999640676\n",
            "[63, 78700] loss: 0.394 accuracy: 74 %\n",
            "lr 0.0003588087549336204\n",
            "[64, 79150] loss: 0.400 accuracy: 74 %\n",
            "lr 0.0003582303421099767\n",
            "[64, 79550] loss: 0.394 accuracy: 75 %\n",
            "lr 0.0003577177606868181\n",
            "[64, 79950] loss: 0.412 accuracy: 74 %\n",
            "lr 0.0003572066440435792\n",
            "[65, 80400] loss: 0.394 accuracy: 74 %\n",
            "lr 0.00035663338088445074\n",
            "[65, 80800] loss: 0.385 accuracy: 74 %\n",
            "lr 0.00035612535612535614\n",
            "[65, 81200] loss: 0.404 accuracy: 74 %\n",
            "lr 0.0003556187766714082\n",
            "[66, 81650] loss: 0.387 accuracy: 74 %\n",
            "lr 0.00035505059470974617\n",
            "[66, 82050] loss: 0.391 accuracy: 75 %\n",
            "lr 0.0003545470661230278\n",
            "[66, 82450] loss: 0.392 accuracy: 74 %\n",
            "lr 0.00035404496371039125\n",
            "[67, 82900] loss: 0.376 accuracy: 74 %\n",
            "lr 0.00035348179568752205\n",
            "[67, 83300] loss: 0.382 accuracy: 74 %\n",
            "lr 0.00035298270384751147\n",
            "[67, 83700] loss: 0.386 accuracy: 74 %\n",
            "lr 0.00035248501938667606\n",
            "[68, 84150] loss: 0.388 accuracy: 74 %\n",
            "lr 0.00035192679922576106\n",
            "[68, 84550] loss: 0.387 accuracy: 74 %\n",
            "lr 0.0003514320857494289\n",
            "[68, 84950] loss: 0.385 accuracy: 74 %\n",
            "lr 0.00035093876118617303\n",
            "[69, 85400] loss: 0.384 accuracy: 74 %\n",
            "lr 0.000350385423966363\n",
            "[69, 85800] loss: 0.396 accuracy: 74 %\n",
            "lr 0.0003498950314905528\n",
            "[69, 86200] loss: 0.392 accuracy: 75 %\n",
            "lr 0.00034940600978336826\n",
            "[70, 86650] loss: 0.370 accuracy: 75 %\n",
            "lr 0.00034885749171463456\n",
            "[70, 87050] loss: 0.373 accuracy: 75 %\n",
            "lr 0.0003483713638738896\n",
            "[70, 87450] loss: 0.376 accuracy: 74 %\n",
            "lr 0.0003478865889719951\n",
            "[71, 87900] loss: 0.370 accuracy: 74 %\n",
            "lr 0.0003473428273706148\n",
            "[71, 88300] loss: 0.370 accuracy: 75 %\n",
            "lr 0.000346860908775581\n",
            "[71, 88700] loss: 0.374 accuracy: 75 %\n",
            "lr 0.00034638032559750607\n",
            "[72, 89150] loss: 0.386 accuracy: 75 %\n",
            "lr 0.00034584125886218223\n",
            "[72, 89550] loss: 0.359 accuracy: 74 %\n",
            "lr 0.00034536349507857016\n",
            "[72, 89950] loss: 0.380 accuracy: 75 %\n",
            "lr 0.0003448870494912916\n",
            "[73, 90400] loss: 0.384 accuracy: 74 %\n",
            "lr 0.0003443526170798898\n",
            "[73, 90800] loss: 0.379 accuracy: 75 %\n",
            "lr 0.000343878954607978\n",
            "[73, 91200] loss: 0.374 accuracy: 75 %\n",
            "lr 0.00034340659340659343\n",
            "[74, 91650] loss: 0.360 accuracy: 75 %\n",
            "lr 0.000342876735813475\n",
            "[74, 92050] loss: 0.380 accuracy: 74 %\n",
            "lr 0.000342407122068139\n",
            "[74, 92450] loss: 0.365 accuracy: 75 %\n",
            "lr 0.00034193879295606086\n",
            "[75, 92900] loss: 0.357 accuracy: 75 %\n",
            "lr 0.00034141345168999654\n",
            "[75, 93300] loss: 0.372 accuracy: 74 %\n",
            "lr 0.0003409478349812479\n",
            "[75, 93700] loss: 0.359 accuracy: 75 %\n",
            "lr 0.00034048348655090226\n",
            "[76, 94150] loss: 0.363 accuracy: 75 %\n",
            "lr 0.0003399626041135475\n",
            "[76, 94550] loss: 0.362 accuracy: 74 %\n",
            "lr 0.00033950093362756747\n",
            "[76, 94950] loss: 0.373 accuracy: 75 %\n",
            "lr 0.00033904051534158335\n",
            "[77, 95400] loss: 0.367 accuracy: 74 %\n",
            "lr 0.00033852403520649965\n",
            "[77, 95800] loss: 0.355 accuracy: 74 %\n",
            "lr 0.0003380662609871535\n",
            "[77, 96200] loss: 0.371 accuracy: 74 %\n",
            "lr 0.000337609723160027\n",
            "[78, 96650] loss: 0.363 accuracy: 75 %\n",
            "lr 0.0003370975897522333\n",
            "[78, 97050] loss: 0.357 accuracy: 75 %\n",
            "lr 0.00033664366268305\n",
            "[78, 97450] loss: 0.363 accuracy: 75 %\n",
            "lr 0.00033619095646327115\n",
            "[79, 97900] loss: 0.354 accuracy: 75 %\n",
            "lr 0.0003356831151393085\n",
            "[79, 98300] loss: 0.356 accuracy: 75 %\n",
            "lr 0.00033523298692591353\n",
            "[79, 98700] loss: 0.343 accuracy: 75 %\n",
            "lr 0.00033478406427854036\n",
            "[80, 99150] loss: 0.354 accuracy: 74 %\n",
            "lr 0.0003342804613070366\n",
            "[80, 99550] loss: 0.363 accuracy: 75 %\n",
            "lr 0.00033383408446002337\n",
            "[80, 99950] loss: 0.357 accuracy: 74 %\n",
            "lr 0.0003333888981496916\n",
            "[81, 100400] loss: 0.349 accuracy: 74 %\n",
            "lr 0.00033288948069241014\n",
            "[81, 100800] loss: 0.354 accuracy: 74 %\n",
            "lr 0.0003324468085106383\n",
            "[81, 101200] loss: 0.354 accuracy: 75 %\n",
            "lr 0.0003320053120849934\n",
            "[82, 101650] loss: 0.348 accuracy: 75 %\n",
            "lr 0.00033151002817835237\n",
            "[82, 102050] loss: 0.357 accuracy: 75 %\n",
            "lr 0.00033107101473266014\n",
            "[82, 102450] loss: 0.359 accuracy: 75 %\n",
            "lr 0.00033063316250619935\n",
            "[83, 102900] loss: 0.337 accuracy: 75 %\n",
            "lr 0.0003301419610432486\n",
            "[83, 103300] loss: 0.355 accuracy: 75 %\n",
            "lr 0.00032970656116056705\n",
            "[83, 103700] loss: 0.356 accuracy: 75 %\n",
            "lr 0.0003292723081988805\n",
            "[84, 104150] loss: 0.339 accuracy: 74 %\n",
            "lr 0.0003287851389117212\n",
            "[84, 104550] loss: 0.357 accuracy: 75 %\n",
            "lr 0.0003283533081595797\n",
            "[84, 104950] loss: 0.341 accuracy: 75 %\n",
            "lr 0.0003279226102639777\n",
            "[85, 105400] loss: 0.356 accuracy: 75 %\n",
            "lr 0.00032743942370661423\n",
            "[85, 105800] loss: 0.357 accuracy: 75 %\n",
            "lr 0.0003270111183780249\n",
            "[85, 106200] loss: 0.345 accuracy: 75 %\n",
            "lr 0.0003265839320705421\n",
            "[86, 106650] loss: 0.336 accuracy: 75 %\n",
            "lr 0.0003261046796021523\n",
            "[86, 107050] loss: 0.345 accuracy: 75 %\n",
            "lr 0.0003256798567008631\n",
            "[86, 107450] loss: 0.344 accuracy: 75 %\n",
            "lr 0.0003252561392096276\n",
            "[87, 107900] loss: 0.335 accuracy: 75 %\n",
            "lr 0.0003247807729782397\n",
            "[87, 108300] loss: 0.348 accuracy: 75 %\n",
            "lr 0.0003243593902043464\n",
            "[87, 108700] loss: 0.354 accuracy: 74 %\n",
            "lr 0.00032393909944930353\n",
            "[88, 109150] loss: 0.336 accuracy: 76 %\n",
            "lr 0.00032346757237586933\n",
            "[88, 109550] loss: 0.344 accuracy: 75 %\n",
            "lr 0.0003230495881117751\n",
            "[88, 109950] loss: 0.342 accuracy: 75 %\n",
            "lr 0.00032263268269075657\n",
            "[89, 110400] loss: 0.335 accuracy: 75 %\n",
            "lr 0.00032216494845360824\n",
            "[89, 110800] loss: 0.348 accuracy: 75 %\n",
            "lr 0.00032175032175032174\n",
            "[89, 111200] loss: 0.335 accuracy: 75 %\n",
            "lr 0.00032133676092544985\n",
            "[90, 111650] loss: 0.331 accuracy: 75 %\n",
            "lr 0.00032087277394513073\n",
            "[90, 112050] loss: 0.330 accuracy: 75 %\n",
            "lr 0.00032046146450889285\n",
            "[90, 112450] loss: 0.335 accuracy: 75 %\n",
            "lr 0.0003200512081933109\n",
            "[91, 112900] loss: 0.350 accuracy: 75 %\n",
            "lr 0.00031959092361776926\n",
            "[91, 113300] loss: 0.339 accuracy: 76 %\n",
            "lr 0.0003191828917969997\n",
            "[91, 113700] loss: 0.331 accuracy: 75 %\n",
            "lr 0.000318775900541919\n",
            "[92, 114150] loss: 0.334 accuracy: 76 %\n",
            "lr 0.0003183192742320547\n",
            "[92, 114550] loss: 0.338 accuracy: 75 %\n",
            "lr 0.00031791448100460976\n",
            "[92, 114950] loss: 0.335 accuracy: 76 %\n",
            "lr 0.00031751071598666456\n",
            "[93, 115400] loss: 0.328 accuracy: 75 %\n",
            "lr 0.0003170577045022194\n",
            "[93, 115800] loss: 0.333 accuracy: 75 %\n",
            "lr 0.0003166561114629512\n",
            "[93, 116200] loss: 0.327 accuracy: 75 %\n",
            "lr 0.00031625553447185326\n",
            "[94, 116650] loss: 0.332 accuracy: 76 %\n",
            "lr 0.00031580609505763463\n",
            "[94, 117050] loss: 0.330 accuracy: 75 %\n",
            "lr 0.00031540766440624505\n",
            "[94, 117450] loss: 0.340 accuracy: 76 %\n",
            "lr 0.00031501023783272954\n",
            "[95, 117900] loss: 0.324 accuracy: 75 %\n",
            "lr 0.00031456432840515884\n",
            "[95, 118300] loss: 0.327 accuracy: 76 %\n",
            "lr 0.0003141690229343387\n",
            "[95, 118700] loss: 0.330 accuracy: 75 %\n",
            "lr 0.00031377470975839345\n",
            "[96, 119150] loss: 0.332 accuracy: 75 %\n",
            "lr 0.00031333228889237035\n",
            "[96, 119550] loss: 0.333 accuracy: 76 %\n",
            "lr 0.00031294007197621654\n",
            "[96, 119950] loss: 0.317 accuracy: 76 %\n",
            "lr 0.0003125488357555868\n",
            "[97, 120400] loss: 0.328 accuracy: 75 %\n",
            "lr 0.00031210986267166043\n",
            "[97, 120800] loss: 0.330 accuracy: 76 %\n",
            "lr 0.0003117206982543641\n",
            "[97, 121200] loss: 0.327 accuracy: 75 %\n",
            "lr 0.00031133250311332503\n",
            "[98, 121650] loss: 0.332 accuracy: 75 %\n",
            "lr 0.000310896937665164\n",
            "[98, 122050] loss: 0.329 accuracy: 75 %\n",
            "lr 0.0003105107902499612\n",
            "[98, 122450] loss: 0.329 accuracy: 75 %\n",
            "lr 0.0003101256008683517\n",
            "[99, 122900] loss: 0.326 accuracy: 75 %\n",
            "lr 0.0003096934035305048\n",
            "[99, 123300] loss: 0.326 accuracy: 75 %\n",
            "lr 0.0003093102381688834\n",
            "[99, 123700] loss: 0.314 accuracy: 75 %\n",
            "lr 0.00030892801977139327\n",
            "[100, 124150] loss: 0.316 accuracy: 75 %\n",
            "lr 0.000308499151627333\n",
            "[100, 124550] loss: 0.321 accuracy: 75 %\n",
            "lr 0.0003081189339084887\n",
            "[100, 124950] loss: 0.324 accuracy: 76 %\n",
            "lr 0.00030773965225419295\n",
            "[101, 125400] loss: 0.322 accuracy: 75 %\n",
            "lr 0.0003073140749846343\n",
            "[101, 125800] loss: 0.326 accuracy: 75 %\n",
            "lr 0.00030693677102516884\n",
            "[101, 126200] loss: 0.323 accuracy: 75 %\n",
            "lr 0.00030656039239730225\n",
            "[102, 126650] loss: 0.307 accuracy: 75 %\n",
            "lr 0.00030613806826878924\n",
            "[102, 127050] loss: 0.326 accuracy: 75 %\n",
            "lr 0.00030576364470264486\n",
            "[102, 127450] loss: 0.313 accuracy: 76 %\n",
            "lr 0.00030539013589861045\n",
            "[103, 127900] loss: 0.317 accuracy: 75 %\n",
            "lr 0.0003049710277523635\n",
            "[103, 128300] loss: 0.313 accuracy: 76 %\n",
            "lr 0.00030459945172098686\n",
            "[103, 128700] loss: 0.325 accuracy: 75 %\n",
            "lr 0.000304228780042592\n",
            "[104, 129150] loss: 0.309 accuracy: 75 %\n",
            "lr 0.0003038128512836093\n",
            "[104, 129550] loss: 0.317 accuracy: 75 %\n",
            "lr 0.00030344409042633893\n",
            "[104, 129950] loss: 0.320 accuracy: 76 %\n",
            "lr 0.00030307622367025305\n",
            "[105, 130400] loss: 0.301 accuracy: 75 %\n",
            "lr 0.00030266343825665856\n",
            "[105, 130800] loss: 0.315 accuracy: 75 %\n",
            "lr 0.00030229746070133015\n",
            "[105, 131200] loss: 0.320 accuracy: 75 %\n",
            "lr 0.00030193236714975844\n",
            "[106, 131650] loss: 0.307 accuracy: 76 %\n",
            "lr 0.00030152268958239106\n",
            "[106, 132050] loss: 0.312 accuracy: 76 %\n",
            "lr 0.0003011594639361542\n",
            "[106, 132450] loss: 0.317 accuracy: 76 %\n",
            "lr 0.00030079711234772146\n",
            "[107, 132900] loss: 0.307 accuracy: 75 %\n",
            "lr 0.00030039050765995795\n",
            "[107, 133300] loss: 0.309 accuracy: 76 %\n",
            "lr 0.0003000300030003\n",
            "[107, 133700] loss: 0.314 accuracy: 76 %\n",
            "lr 0.0002996703626011387\n",
            "[108, 134150] loss: 0.306 accuracy: 75 %\n",
            "lr 0.0002992667963489451\n",
            "[108, 134550] loss: 0.321 accuracy: 76 %\n",
            "lr 0.00029890898221491554\n",
            "[108, 134950] loss: 0.317 accuracy: 75 %\n",
            "lr 0.00029855202268995375\n",
            "[109, 135400] loss: 0.299 accuracy: 76 %\n",
            "lr 0.0002981514609421586\n",
            "[109, 135800] loss: 0.312 accuracy: 76 %\n",
            "lr 0.00029779630732578913\n",
            "[109, 136200] loss: 0.313 accuracy: 76 %\n",
            "lr 0.000297441998810232\n",
            "[110, 136650] loss: 0.312 accuracy: 75 %\n",
            "lr 0.0002970444081390168\n",
            "[110, 137050] loss: 0.308 accuracy: 76 %\n",
            "lr 0.00029669188547693225\n",
            "[110, 137450] loss: 0.314 accuracy: 76 %\n",
            "lr 0.000296340198547933\n",
            "[111, 137900] loss: 0.315 accuracy: 76 %\n",
            "lr 0.0002959455460195324\n",
            "[111, 138300] loss: 0.313 accuracy: 76 %\n",
            "lr 0.0002955956251847473\n",
            "[111, 138700] loss: 0.309 accuracy: 76 %\n",
            "lr 0.0002952465308532625\n",
            "[112, 139150] loss: 0.310 accuracy: 76 %\n",
            "lr 0.0002948547840188707\n",
            "[112, 139550] loss: 0.317 accuracy: 76 %\n",
            "lr 0.0002945074363127669\n",
            "[112, 139950] loss: 0.307 accuracy: 76 %\n",
            "lr 0.00029416090601559053\n",
            "[113, 140400] loss: 0.306 accuracy: 76 %\n",
            "lr 0.0002937720329024677\n",
            "[113, 140800] loss: 0.309 accuracy: 76 %\n",
            "lr 0.0002934272300469483\n",
            "[113, 141200] loss: 0.311 accuracy: 76 %\n",
            "lr 0.0002930832356389215\n",
            "[114, 141650] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002926972047416947\n",
            "[114, 142050] loss: 0.296 accuracy: 76 %\n",
            "lr 0.00029235491887151\n",
            "[114, 142450] loss: 0.311 accuracy: 76 %\n",
            "lr 0.0002920134326179004\n",
            "[115, 142900] loss: 0.299 accuracy: 76 %\n",
            "lr 0.0002916302128900554\n",
            "[115, 143300] loss: 0.303 accuracy: 76 %\n",
            "lr 0.0002912904165452957\n",
            "[115, 143700] loss: 0.304 accuracy: 76 %\n",
            "lr 0.00029095141111434386\n",
            "[116, 144150] loss: 0.297 accuracy: 76 %\n",
            "lr 0.00029057097195990116\n",
            "[116, 144550] loss: 0.301 accuracy: 76 %\n",
            "lr 0.0002902336380786533\n",
            "[116, 144950] loss: 0.306 accuracy: 76 %\n",
            "lr 0.00028989708653428034\n",
            "[117, 145400] loss: 0.302 accuracy: 76 %\n",
            "lr 0.00028951939779965256\n",
            "[117, 145800] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002891844997108155\n",
            "[117, 146200] loss: 0.310 accuracy: 76 %\n",
            "lr 0.00028885037550548814\n",
            "[118, 146650] loss: 0.299 accuracy: 75 %\n",
            "lr 0.0002884754074715131\n",
            "[118, 147050] loss: 0.293 accuracy: 76 %\n",
            "lr 0.0002881429188877683\n",
            "[118, 147450] loss: 0.304 accuracy: 76 %\n",
            "lr 0.0002878111958555188\n",
            "[119, 147900] loss: 0.297 accuracy: 76 %\n",
            "lr 0.0002874389192296637\n",
            "[119, 148300] loss: 0.292 accuracy: 76 %\n",
            "lr 0.0002871088142405972\n",
            "[119, 148700] loss: 0.303 accuracy: 75 %\n",
            "lr 0.00028677946659019213\n",
            "[120, 149150] loss: 0.298 accuracy: 76 %\n",
            "lr 0.00028640985249892595\n",
            "[120, 149550] loss: 0.294 accuracy: 75 %\n",
            "lr 0.000286082105564297\n",
            "[120, 149950] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002857551078725532\n",
            "[121, 150400] loss: 0.292 accuracy: 76 %\n",
            "lr 0.00028538812785388126\n",
            "[121, 150800] loss: 0.303 accuracy: 76 %\n",
            "lr 0.00028506271379703536\n",
            "[121, 151200] loss: 0.297 accuracy: 76 %\n",
            "lr 0.0002847380410022779\n",
            "[122, 151650] loss: 0.305 accuracy: 76 %\n",
            "lr 0.00028437366699843596\n",
            "[122, 152050] loss: 0.282 accuracy: 76 %\n",
            "lr 0.00028405056099985794\n",
            "[122, 152450] loss: 0.307 accuracy: 76 %\n",
            "lr 0.0002837281883955171\n",
            "[123, 152900] loss: 0.303 accuracy: 76 %\n",
            "lr 0.00028336639274582036\n",
            "[123, 153300] loss: 0.300 accuracy: 76 %\n",
            "lr 0.0002830455703368242\n",
            "[123, 153700] loss: 0.293 accuracy: 76 %\n",
            "lr 0.00028272547356516825\n",
            "[124, 154150] loss: 0.295 accuracy: 76 %\n",
            "lr 0.00028236622899901174\n",
            "[124, 154550] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002820476660555634\n",
            "[124, 154950] loss: 0.294 accuracy: 76 %\n",
            "lr 0.00028172982110156357\n",
            "[125, 155400] loss: 0.288 accuracy: 76 %\n",
            "lr 0.00028137310073157\n",
            "[125, 155800] loss: 0.304 accuracy: 76 %\n",
            "lr 0.0002810567734682406\n",
            "[125, 156200] loss: 0.298 accuracy: 76 %\n",
            "lr 0.0002807411566535654\n",
            "[126, 156650] loss: 0.290 accuracy: 76 %\n",
            "lr 0.00028038693396887705\n",
            "[126, 157050] loss: 0.291 accuracy: 76 %\n",
            "lr 0.0002800728189329226\n",
            "[126, 157450] loss: 0.293 accuracy: 76 %\n",
            "lr 0.00027975940691005733\n",
            "[127, 157900] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002794076557697681\n",
            "[127, 158300] loss: 0.287 accuracy: 76 %\n",
            "lr 0.00027909572983533354\n",
            "[127, 158700] loss: 0.297 accuracy: 76 %\n",
            "lr 0.00027878449958182325\n",
            "[128, 159150] loss: 0.293 accuracy: 76 %\n",
            "lr 0.00027843519420854795\n",
            "[128, 159550] loss: 0.295 accuracy: 76 %\n",
            "lr 0.0002781254345709915\n",
            "[128, 159950] loss: 0.285 accuracy: 76 %\n",
            "lr 0.0002778163633838033\n",
            "[129, 160400] loss: 0.302 accuracy: 76 %\n",
            "lr 0.0002774694783573807\n",
            "[129, 160800] loss: 0.280 accuracy: 76 %\n",
            "lr 0.0002771618625277162\n",
            "[129, 161200] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002768549280177187\n",
            "[130, 161650] loss: 0.292 accuracy: 76 %\n",
            "lr 0.00027651043826904464\n",
            "[130, 162050] loss: 0.287 accuracy: 76 %\n",
            "lr 0.00027620494406849883\n",
            "[130, 162450] loss: 0.285 accuracy: 76 %\n",
            "lr 0.00027590012415505585\n",
            "[131, 162900] loss: 0.279 accuracy: 76 %\n",
            "lr 0.00027555800496004406\n",
            "[131, 163300] loss: 0.285 accuracy: 76 %\n",
            "lr 0.0002752546105147261\n",
            "[131, 163700] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002749518834204014\n",
            "[132, 164150] loss: 0.277 accuracy: 76 %\n",
            "lr 0.0002746121103940684\n",
            "[132, 164550] loss: 0.286 accuracy: 76 %\n",
            "lr 0.000274310794129749\n",
            "[132, 164950] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002740101383751199\n",
            "[133, 165400] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002736726874657909\n",
            "[133, 165800] loss: 0.294 accuracy: 76 %\n",
            "lr 0.0002733734281027884\n",
            "[133, 166200] loss: 0.289 accuracy: 76 %\n",
            "lr 0.0002730748225013654\n",
            "[134, 166650] loss: 0.276 accuracy: 76 %\n",
            "lr 0.0002727396699849993\n",
            "[134, 167050] loss: 0.290 accuracy: 76 %\n",
            "lr 0.0002724424465331699\n",
            "[134, 167450] loss: 0.287 accuracy: 76 %\n",
            "lr 0.00027214587018641994\n",
            "[135, 167900] loss: 0.284 accuracy: 76 %\n",
            "lr 0.0002718129926610492\n",
            "[135, 168300] loss: 0.292 accuracy: 76 %\n",
            "lr 0.00027151778441487917\n",
            "[135, 168700] loss: 0.285 accuracy: 76 %\n",
            "lr 0.00027122321670735016\n",
            "[136, 169150] loss: 0.278 accuracy: 76 %\n",
            "lr 0.0002708925910876337\n",
            "[136, 169550] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00027059937762143147\n",
            "[136, 169950] loss: 0.283 accuracy: 76 %\n",
            "lr 0.0002703067982159751\n",
            "[137, 170400] loss: 0.284 accuracy: 76 %\n",
            "lr 0.0002699784017278618\n",
            "[137, 170800] loss: 0.277 accuracy: 76 %\n",
            "lr 0.00026968716289104636\n",
            "[137, 171200] loss: 0.281 accuracy: 76 %\n",
            "lr 0.00026939655172413793\n",
            "[138, 171650] loss: 0.273 accuracy: 77 %\n",
            "lr 0.0002690703618996368\n",
            "[138, 172050] loss: 0.281 accuracy: 76 %\n",
            "lr 0.000268781077812122\n",
            "[138, 172450] loss: 0.281 accuracy: 76 %\n",
            "lr 0.0002684924150892737\n",
            "[139, 172900] loss: 0.280 accuracy: 76 %\n",
            "lr 0.0002681684097613301\n",
            "[139, 173300] loss: 0.279 accuracy: 76 %\n",
            "lr 0.0002678810608090008\n",
            "[139, 173700] loss: 0.271 accuracy: 76 %\n",
            "lr 0.0002675943270002676\n",
            "[140, 174150] loss: 0.284 accuracy: 76 %\n",
            "lr 0.00026727248429774154\n",
            "[140, 174550] loss: 0.282 accuracy: 76 %\n",
            "lr 0.0002669870511280203\n",
            "[140, 174950] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002667022269635951\n",
            "[141, 175400] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002663825253063399\n",
            "[141, 175800] loss: 0.277 accuracy: 76 %\n",
            "lr 0.00026609898882384245\n",
            "[141, 176200] loss: 0.291 accuracy: 76 %\n",
            "lr 0.00026581605528973947\n",
            "[142, 176650] loss: 0.285 accuracy: 76 %\n",
            "lr 0.00026549847338377806\n",
            "[142, 177050] loss: 0.273 accuracy: 76 %\n",
            "lr 0.0002652168147460549\n",
            "[142, 177450] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00026493575307987813\n",
            "[143, 177900] loss: 0.277 accuracy: 76 %\n",
            "lr 0.0002646202699126753\n",
            "[143, 178300] loss: 0.273 accuracy: 76 %\n",
            "lr 0.0002643404705260375\n",
            "[143, 178700] loss: 0.281 accuracy: 76 %\n",
            "lr 0.0002640612622128334\n",
            "[144, 179150] loss: 0.285 accuracy: 76 %\n",
            "lr 0.00026374785704866147\n",
            "[144, 179550] loss: 0.268 accuracy: 76 %\n",
            "lr 0.00026346989856408903\n",
            "[144, 179950] loss: 0.280 accuracy: 76 %\n",
            "lr 0.00026319252533228054\n",
            "[145, 180400] loss: 0.271 accuracy: 76 %\n",
            "lr 0.0002628811777076761\n",
            "[145, 180800] loss: 0.276 accuracy: 76 %\n",
            "lr 0.00026260504201680677\n",
            "[145, 181200] loss: 0.275 accuracy: 76 %\n",
            "lr 0.00026232948583420777\n",
            "[146, 181650] loss: 0.271 accuracy: 76 %\n",
            "lr 0.0002620201755535176\n",
            "[146, 182050] loss: 0.278 accuracy: 76 %\n",
            "lr 0.00026174584478471405\n",
            "[146, 182450] loss: 0.280 accuracy: 76 %\n",
            "lr 0.0002614720878546215\n",
            "[147, 182900] loss: 0.277 accuracy: 76 %\n",
            "lr 0.00026116479498563595\n",
            "[147, 183300] loss: 0.269 accuracy: 77 %\n",
            "lr 0.00026089225150013044\n",
            "[147, 183700] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00026062027625749283\n",
            "[148, 184150] loss: 0.257 accuracy: 76 %\n",
            "lr 0.0002603149811271639\n",
            "[148, 184550] loss: 0.266 accuracy: 76 %\n",
            "lr 0.0002600442075152776\n",
            "[148, 184950] loss: 0.263 accuracy: 76 %\n",
            "lr 0.0002597739966229381\n",
            "[149, 185400] loss: 0.269 accuracy: 77 %\n",
            "lr 0.0002594706798131811\n",
            "[149, 185800] loss: 0.277 accuracy: 76 %\n",
            "lr 0.0002592016588906169\n",
            "[149, 186200] loss: 0.285 accuracy: 76 %\n",
            "lr 0.0002589331952356292\n",
            "[150, 186650] loss: 0.263 accuracy: 76 %\n",
            "lr 0.00025863183757920597\n",
            "[150, 187050] loss: 0.273 accuracy: 76 %\n",
            "lr 0.000258364552383413\n",
            "[150, 187450] loss: 0.267 accuracy: 76 %\n",
            "lr 0.0002580978190734288\n",
            "[151, 187900] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00025779840164990973\n",
            "[151, 188300] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002575328354365182\n",
            "[151, 188700] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00025726781579624386\n",
            "[152, 189150] loss: 0.260 accuracy: 76 %\n",
            "lr 0.00025697031992804833\n",
            "[152, 189550] loss: 0.267 accuracy: 76 %\n",
            "lr 0.0002567064561673726\n",
            "[152, 189950] loss: 0.257 accuracy: 76 %\n",
            "lr 0.00025644313373509426\n",
            "[153, 190400] loss: 0.270 accuracy: 76 %\n",
            "lr 0.00025614754098360657\n",
            "[153, 190800] loss: 0.267 accuracy: 76 %\n",
            "lr 0.00025588536335721597\n",
            "[153, 191200] loss: 0.272 accuracy: 76 %\n",
            "lr 0.0002556237218813906\n",
            "[154, 191650] loss: 0.268 accuracy: 76 %\n",
            "lr 0.00025533001404315077\n",
            "[154, 192050] loss: 0.261 accuracy: 76 %\n",
            "lr 0.00025506950644050506\n",
            "[154, 192450] loss: 0.272 accuracy: 77 %\n",
            "lr 0.00025480952987641737\n",
            "[155, 192900] loss: 0.252 accuracy: 76 %\n",
            "lr 0.00025451768897938407\n",
            "[155, 193300] loss: 0.265 accuracy: 76 %\n",
            "lr 0.00025425883549453347\n",
            "[155, 193700] loss: 0.264 accuracy: 76 %\n",
            "lr 0.000254000508001016\n",
            "[156, 194150] loss: 0.256 accuracy: 76 %\n",
            "lr 0.00025371051630090067\n",
            "[156, 194550] loss: 0.274 accuracy: 76 %\n",
            "lr 0.0002534533012292485\n",
            "[156, 194950] loss: 0.263 accuracy: 76 %\n",
            "lr 0.000253196607165464\n",
            "[157, 195400] loss: 0.266 accuracy: 76 %\n",
            "lr 0.00025290844714213456\n"
          ]
        }
      ],
      "source": [
        "trainer.train(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIW4yMXrdmyb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07524fa6ad4f479c9078b8b5b8f00b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91cd76a4cd1242b69da3431b3cd85fea",
              "IPY_MODEL_b17ccff517aa43a3893a9f1245de4a5d",
              "IPY_MODEL_b5309cae064b4f3783e3ab4759315716"
            ],
            "layout": "IPY_MODEL_bef19993c0014b5394f5924659d97043"
          }
        },
        "307c6d6782eb4b1fb250c2e5595c65f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a3a02cba8c43bf9e9e25e8d200729b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91cd76a4cd1242b69da3431b3cd85fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a3a02cba8c43bf9e9e25e8d200729b",
            "placeholder": "​",
            "style": "IPY_MODEL_a532249d2c204791bef03c6d21548601",
            "value": "100%"
          }
        },
        "a532249d2c204791bef03c6d21548601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b17ccff517aa43a3893a9f1245de4a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff059c7d378040a599c6cc2c14db1ce8",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8917e7a32474d829b3711c31b9179b9",
            "value": 170498071
          }
        },
        "b5309cae064b4f3783e3ab4759315716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_307c6d6782eb4b1fb250c2e5595c65f3",
            "placeholder": "​",
            "style": "IPY_MODEL_e994c2fa55e84eb19c77b2e911c532a2",
            "value": " 170498071/170498071 [00:11&lt;00:00, 15851362.59it/s]"
          }
        },
        "bef19993c0014b5394f5924659d97043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8917e7a32474d829b3711c31b9179b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e994c2fa55e84eb19c77b2e911c532a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff059c7d378040a599c6cc2c14db1ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}